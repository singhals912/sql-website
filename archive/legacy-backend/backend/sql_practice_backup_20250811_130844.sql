--
-- PostgreSQL database dump
--

-- Dumped from database version 14.18 (Homebrew)
-- Dumped by pg_dump version 14.18 (Homebrew)

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: categories; Type: TABLE; Schema: public; Owner: ss
--

CREATE TABLE public.categories (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    name character varying(100) NOT NULL,
    slug character varying(100) NOT NULL,
    description text,
    created_at timestamp without time zone DEFAULT now()
);


ALTER TABLE public.categories OWNER TO ss;

--
-- Name: problem_schemas; Type: TABLE; Schema: public; Owner: ss
--

CREATE TABLE public.problem_schemas (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    problem_id uuid,
    sql_dialect character varying(50) DEFAULT 'postgresql'::character varying,
    setup_sql text NOT NULL,
    solution_sql text NOT NULL,
    explanation text,
    created_at timestamp without time zone DEFAULT now(),
    expected_output jsonb
);


ALTER TABLE public.problem_schemas OWNER TO ss;

--
-- Name: problems; Type: TABLE; Schema: public; Owner: ss
--

CREATE TABLE public.problems (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    title character varying(200) NOT NULL,
    slug character varying(200) NOT NULL,
    difficulty character varying(20) NOT NULL,
    description text NOT NULL,
    is_premium boolean DEFAULT false,
    is_active boolean DEFAULT true,
    category_id uuid,
    created_at timestamp without time zone DEFAULT now(),
    total_submissions integer DEFAULT 0,
    total_accepted integer DEFAULT 0,
    acceptance_rate numeric(5,2) DEFAULT 0.00,
    tags text[],
    hints text[],
    CONSTRAINT problems_difficulty_check CHECK (((difficulty)::text = ANY ((ARRAY['easy'::character varying, 'medium'::character varying, 'hard'::character varying])::text[])))
);


ALTER TABLE public.problems OWNER TO ss;

--
-- Data for Name: categories; Type: TABLE DATA; Schema: public; Owner: ss
--

COPY public.categories (id, name, slug, description, created_at) FROM stdin;
c5ec99f8-01ff-4d36-b36e-27688566397d	Basic Queries	basic-queries	Fundamental SQL operations	2025-08-11 09:44:19.649811
426fcc68-b403-458f-9afd-5137f772de78	Aggregation	aggregation	GROUP BY, HAVING, aggregate functions	2025-08-11 09:44:19.649811
c7e4c5a1-5b75-4117-a113-118749434557	Advanced Topics	advanced-topics	Complex SQL concepts	2025-08-11 09:44:19.649811
8798fdcf-0411-45cb-83dd-b4912e133354	Joins	joins	INNER, LEFT, RIGHT, FULL OUTER joins	2025-08-11 09:44:19.649811
47c2009b-81d2-458f-96b0-1a68aee370d6	Time Analysis	time-analysis	Date/time functions and analysis	2025-08-11 09:44:19.649811
9ba6536c-e307-41f7-8ae0-8e49f3f98d55	Window Functions	window-functions	ROW_NUMBER, RANK, LAG, LEAD	2025-08-11 09:44:19.649811
e1b879e5-e95b-41ee-b22a-a2ea91897277	Subqueries	subqueries	Correlated and non-correlated subqueries	2025-08-11 09:44:19.649811
089e178f-9baa-41c3-8ed5-626d2f2227d5	Fraud Detection	fraud-detection	Advanced problems in Fraud Detection	2025-08-11 09:51:09.988697
c6117091-f4fa-4c45-925b-8671834d6408	Recommendation Systems	recommendation-systems	Advanced problems in Recommendation Systems	2025-08-11 09:51:10.018351
949357d0-c57c-45fc-93a7-4c350616a3a8	Supply Chain	supply-chain	Advanced problems in Supply Chain	2025-08-11 09:51:10.024129
8a31828d-d2e8-44b8-bb9e-636d039bdbcf	A/B Testing	a/b-testing	Advanced problems in A/B Testing	2025-08-11 09:51:10.028412
672071b9-615d-473b-8985-5fbc5612729e	Energy Analytics	energy-analytics	Advanced problems in Energy Analytics	2025-08-11 09:51:10.032285
\.


--
-- Data for Name: problem_schemas; Type: TABLE DATA; Schema: public; Owner: ss
--

COPY public.problem_schemas (id, problem_id, sql_dialect, setup_sql, solution_sql, explanation, created_at, expected_output) FROM stdin;
d929f63c-5080-4569-b5ae-685e830a46a8	c2db86f4-b45e-4958-bdba-934e9bd8949b	postgresql	\n      CREATE TABLE bookings (\n          booking_id INT,\n          customer_id INT,\n          booking_amount DECIMAL(10,2),\n          booking_date DATE\n      );\n      \n      INSERT INTO bookings VALUES \n      (1, 101, 450.00, '2024-01-15'),\n      (2, 102, 280.00, '2024-01-16'),\n      (3, 101, 620.00, '2024-01-20'),\n      (4, 103, 180.00, '2024-01-18'),\n      (5, 102, 850.00, '2024-01-25'),\n      (6, 104, 320.00, '2024-01-22');\n    	\n      SELECT \n        customer_id,\n        SUM(booking_amount) as total_spent\n      FROM bookings\n      GROUP BY customer_id\n      HAVING SUM(booking_amount) > 1000\n      ORDER BY total_spent DESC;\n    	Solution uses basic queries concepts to solve the business problem.	2025-08-11 09:51:03.998276	[["102", "1130.00"], ["101", "1070.00"]]
f3451e22-b16b-44d6-b99c-85fe86184d81	20fc0621-5497-4fc6-b871-05b2ac57a124	postgresql	\n      CREATE TABLE inventory (\n          product_id INT,\n          product_name VARCHAR(100),\n          quantity INT,\n          reorder_point INT\n      );\n      \n      INSERT INTO inventory VALUES \n      (1, 'Wireless Headphones', 0, 20),\n      (2, 'Bluetooth Speaker', 15, 25),\n      (3, 'Laptop Stand', 75, 30),\n      (4, 'USB Cable', 5, 50),\n      (5, 'Phone Case', 100, 40),\n      (6, 'Screen Protector', 0, 20);\n    	\n      SELECT \n        product_name,\n        quantity,\n        CASE \n          WHEN quantity = 0 THEN 'Out of Stock'\n          WHEN quantity < 50 THEN 'Low Stock'\n          ELSE 'In Stock'\n        END as status\n      FROM inventory\n      ORDER BY quantity;\n    	Solution uses basic queries concepts to solve the business problem.	2025-08-11 09:51:04.03128	[["Wireless Headphones", "0", "Out of Stock"], ["Screen Protector", "0", "Out of Stock"], ["USB Cable", "5", "Low Stock"], ["Bluetooth Speaker", "15", "Low Stock"], ["Laptop Stand", "75", "In Stock"], ["Phone Case", "100", "In Stock"]]
f461f846-e8d1-4e04-9b45-9ffa1c4fade2	38108e8c-7b5c-4134-b346-ea1d7bc31ee2	postgresql	\n      CREATE TABLE posts (\n          post_id INT,\n          user_id INT,\n          post_type VARCHAR(50),\n          likes_count INT,\n          comments_count INT,\n          post_date DATE\n      );\n      \n      INSERT INTO posts VALUES \n      (1, 101, 'article', 45, 12, '2024-01-15'),\n      (2, 102, 'image', 23, 8, '2024-01-16'),\n      (3, 103, 'video', 67, 15, '2024-01-17'),\n      (4, 104, 'article', 34, 6, '2024-01-18'),\n      (5, 105, 'image', 56, 11, '2024-01-19'),\n      (6, 106, 'video', 89, 22, '2024-01-20');\n    	\n      SELECT \n        post_type,\n        ROUND(AVG(likes_count + comments_count), 2) as avg_engagement\n      FROM posts\n      GROUP BY post_type\n      ORDER BY avg_engagement DESC;\n    	Solution uses aggregation concepts to solve the business problem.	2025-08-11 09:51:04.035619	[["video", "96.50"], ["image", "49.00"], ["article", "48.50"]]
dfcec26f-5284-496d-9eb2-7613761d1672	d1092dcc-af01-4f71-9ec6-07be5e714e80	postgresql	\n      CREATE TABLE transactions (\n          transaction_id INT,\n          customer_id INT,\n          amount DECIMAL(10,2),\n          transaction_date DATE\n      );\n      \n      INSERT INTO transactions VALUES \n      (1, 101, 1250.00, '2024-01-15'),\n      (2, 102, 890.00, '2024-01-20'),\n      (3, 103, 2100.00, '2024-01-25'),\n      (4, 101, 650.00, '2024-02-10'),\n      (5, 104, 1800.00, '2024-02-15'),\n      (6, 102, 950.00, '2024-02-20'),\n      (7, 103, 1200.00, '2024-02-25'),\n      (8, 104, 750.00, '2024-03-05');\n    	\n      WITH monthly_spending AS (\n        SELECT \n          TO_CHAR(transaction_date, 'YYYY-MM') as month,\n          customer_id,\n          SUM(amount) as total_spent\n        FROM transactions\n        GROUP BY TO_CHAR(transaction_date, 'YYYY-MM'), customer_id\n      ),\n      ranked_customers AS (\n        SELECT \n          month,\n          customer_id,\n          total_spent,\n          ROW_NUMBER() OVER (PARTITION BY month ORDER BY total_spent DESC) as rank\n        FROM monthly_spending\n      )\n      SELECT month, customer_id, total_spent\n      FROM ranked_customers\n      WHERE rank = 1\n      ORDER BY month;\n    	Solution uses window functions concepts to solve the business problem.	2025-08-11 09:51:04.041038	[["2024-01", "103", "2100.00"], ["2024-02", "104", "1800.00"], ["2024-03", "104", "750.00"]]
b025c02c-9613-436f-9031-c71c3eb98291	5a52f8a8-e4d2-4519-958f-9352010eff9d	postgresql	\n      CREATE TABLE user_sessions (\n          session_id INT,\n          user_id INT,\n          start_time TIMESTAMP,\n          end_time TIMESTAMP,\n          videos_watched INT\n      );\n      \n      INSERT INTO user_sessions VALUES \n      (1, 101, '2024-01-15 14:00:00', '2024-01-15 14:45:00', 3),\n      (2, 102, '2024-01-15 15:00:00', '2024-01-15 15:20:00', 2),\n      (3, 101, '2024-01-16 16:00:00', '2024-01-16 16:35:00', 4),\n      (4, 103, '2024-01-16 17:00:00', '2024-01-16 18:30:00', 6),\n      (5, 102, '2024-01-17 19:00:00', '2024-01-17 19:15:00', 1),\n      (6, 101, '2024-01-17 20:00:00', '2024-01-17 20:40:00', 3),\n      (7, 103, '2024-01-18 14:30:00', '2024-01-18 16:00:00', 8);\n    	\n      SELECT \n        user_id,\n        COUNT(*) as session_count,\n        SUM(EXTRACT(EPOCH FROM (end_time - start_time))/60) as total_minutes,\n        ROUND(AVG(EXTRACT(EPOCH FROM (end_time - start_time))/60), 2) as avg_session_duration\n      FROM user_sessions\n      GROUP BY user_id\n      HAVING AVG(EXTRACT(EPOCH FROM (end_time - start_time))/60) > 30 \n        AND SUM(EXTRACT(EPOCH FROM (end_time - start_time))/60) > 120\n      ORDER BY total_minutes DESC;\n    	Solution uses time analysis concepts to solve the business problem.	2025-08-11 09:51:04.044584	[["103", "2", "180.0000000000000000", "90.00"]]
bf6ffc26-ce6e-4516-8801-d7c0b8901ae3	1b437802-cd3e-4030-901c-74517fd027af	postgresql	\n      CREATE TABLE financial_transactions (\n          transaction_id INT,\n          user_id INT,\n          amount DECIMAL(12,2),\n          transaction_date DATE,\n          merchant_category VARCHAR(50)\n      );\n      \n      INSERT INTO financial_transactions VALUES \n      (1, 101, 45.50, '2024-01-15', 'grocery'),\n      (2, 101, 2500.00, '2024-01-20', 'electronics'),\n      (3, 101, 38.75, '2024-01-25', 'restaurant'),\n      (4, 102, 150.00, '2024-01-16', 'clothing'),\n      (5, 102, 180.25, '2024-01-22', 'gas'),\n      (6, 102, 1200.00, '2024-01-28', 'jewelry'),\n      (7, 103, 25.00, '2024-01-18', 'coffee'),\n      (8, 103, 95.50, '2024-01-24', 'grocery'),\n      (9, 103, 750.00, '2024-01-30', 'electronics');\n    	\n      WITH user_averages AS (\n        SELECT \n          user_id,\n          AVG(amount) as avg_amount\n        FROM financial_transactions\n        GROUP BY user_id\n      ),\n      transaction_analysis AS (\n        SELECT \n          ft.user_id,\n          ft.transaction_id,\n          ft.amount,\n          ua.avg_amount as user_avg_amount,\n          ROUND(ft.amount / ua.avg_amount, 2) as ratio\n        FROM financial_transactions ft\n        JOIN user_averages ua ON ft.user_id = ua.user_id\n      )\n      SELECT *\n      FROM transaction_analysis\n      WHERE ratio > 3.0\n      ORDER BY ratio DESC;\n    	Advanced Fraud Detection problem with real business applications.	2025-08-11 09:51:09.988697	[]
9f4f326f-a392-4dbc-95bf-17731ea6bc8c	b01ac3b6-2598-4cc2-9375-cec6a2aa0560	postgresql	CREATE TABLE azure_usage (customer_id INT, service_type VARCHAR(50), usage_hours DECIMAL(10,2), cost DECIMAL(10,2), usage_date DATE);\n    INSERT INTO azure_usage VALUES (1, 'Virtual Machines', 720.5, 1250.75, '2024-01-01'), (2, 'Storage', 8760.0, 450.25, '2024-01-01'), (3, 'Database', 2160.0, 890.50, '2024-01-01');	SELECT service_type, AVG(cost) as avg_cost, ROW_NUMBER() OVER (ORDER BY AVG(cost) DESC) as cost_rank FROM azure_usage GROUP BY service_type ORDER BY avg_cost DESC;	Window Functions analysis for Microsoft Azure Cloud Analytics	2025-08-11 13:06:42.248499	\N
2f746128-22f2-44aa-8e38-c72b052278be	31ec673c-d80a-4fa5-b27b-2cfb56d5853c	postgresql	\n      CREATE TABLE movies (\n          movie_id INT,\n          title VARCHAR(100),\n          genre VARCHAR(50),\n          release_year INT,\n          avg_rating DECIMAL(3,2)\n      );\n      \n      CREATE TABLE user_views (\n          user_id INT,\n          movie_id INT,\n          rating INT,\n          view_date DATE\n      );\n      \n      INSERT INTO movies VALUES \n      (1, 'Top Gun Maverick', 'Action', 2022, 4.5),\n      (2, 'The Batman', 'Action', 2022, 4.2),\n      (3, 'Frozen 2', 'Animation', 2019, 4.3),\n      (4, 'John Wick 4', 'Action', 2023, 4.4),\n      (5, 'Encanto', 'Animation', 2021, 4.1),\n      (6, 'Fast X', 'Action', 2023, 3.8);\n      \n      INSERT INTO user_views VALUES \n      (101, 1, 5, '2024-01-15'),\n      (101, 3, 4, '2024-01-20'),\n      (102, 1, 4, '2024-01-16'),\n      (102, 2, 5, '2024-01-22'),\n      (103, 3, 5, '2024-01-18'),\n      (103, 5, 4, '2024-01-25');\n    	\n      WITH action_viewers AS (\n        SELECT DISTINCT uv.user_id\n        FROM user_views uv\n        JOIN movies m ON uv.movie_id = m.movie_id\n        WHERE m.genre = 'Action'\n      ),\n      user_watched_movies AS (\n        SELECT user_id, movie_id\n        FROM user_views\n      )\n      SELECT DISTINCT \n        av.user_id,\n        m.movie_id,\n        m.title,\n        m.avg_rating\n      FROM action_viewers av\n      CROSS JOIN movies m\n      LEFT JOIN user_watched_movies uwm ON av.user_id = uwm.user_id AND m.movie_id = uwm.movie_id\n      WHERE m.genre = 'Action'\n        AND uwm.movie_id IS NULL\n        AND m.avg_rating >= 4.0\n      ORDER BY av.user_id, m.avg_rating DESC;\n    	Advanced Recommendation Systems problem with real business applications.	2025-08-11 09:51:10.018351	[["101", "4", "John Wick 4", "4.40"], ["101", "2", "The Batman", "4.20"], ["102", "4", "John Wick 4", "4.40"]]
13047a10-880f-44ba-b6b6-d2be58562d1a	0d48baad-aa87-4f93-9612-328305363c65	postgresql	\n      CREATE TABLE warehouse_inventory (\n          warehouse_id INT,\n          product_name VARCHAR(100),\n          current_stock INT,\n          reorder_point INT,\n          max_capacity INT\n      );\n      \n      INSERT INTO warehouse_inventory VALUES \n      (1, 'Shipping Boxes', 150, 200, 1000),\n      (1, 'Bubble Wrap', 75, 100, 500),\n      (1, 'Packing Tape', 300, 150, 800),\n      (2, 'Shipping Boxes', 180, 200, 1000),\n      (2, 'Bubble Wrap', 45, 100, 500),\n      (2, 'Labels', 250, 300, 600),\n      (3, 'Shipping Boxes', 90, 200, 1000),\n      (3, 'Packing Tape', 120, 150, 800);\n    	\n      SELECT \n        warehouse_id,\n        product_name,\n        current_stock,\n        reorder_point,\n        reorder_point - current_stock as shortage_amount\n      FROM warehouse_inventory\n      WHERE current_stock < reorder_point\n      ORDER BY warehouse_id, shortage_amount DESC;\n    	Advanced Supply Chain problem with real business applications.	2025-08-11 09:51:10.024129	[["1", "Shipping Boxes", "150", "200", "50"], ["1", "Bubble Wrap", "75", "100", "25"], ["2", "Bubble Wrap", "45", "100", "55"], ["2", "Labels", "250", "300", "50"], ["2", "Shipping Boxes", "180", "200", "20"], ["3", "Shipping Boxes", "90", "200", "110"], ["3", "Packing Tape", "120", "150", "30"]]
5edcf6af-0460-485e-bd15-1a15a9d55888	4aeaaea6-093f-41c4-9037-d36e44e11bd1	postgresql	\n      CREATE TABLE ab_test_results (\n          user_id INT,\n          test_group VARCHAR(20),\n          converted BOOLEAN,\n          signup_date DATE\n      );\n      \n      INSERT INTO ab_test_results VALUES \n      (1, 'control', true, '2024-01-15'),\n      (2, 'control', false, '2024-01-16'),\n      (3, 'control', true, '2024-01-17'),\n      (4, 'control', false, '2024-01-18'),\n      (5, 'treatment', true, '2024-01-15'),\n      (6, 'treatment', true, '2024-01-16'),\n      (7, 'treatment', false, '2024-01-17'),\n      (8, 'treatment', true, '2024-01-18'),\n      (9, 'control', false, '2024-01-19'),\n      (10, 'treatment', true, '2024-01-19');\n    	\n      SELECT \n        test_group as group_name,\n        COUNT(*) as total_users,\n        SUM(CASE WHEN converted THEN 1 ELSE 0 END) as conversions,\n        ROUND(\n          SUM(CASE WHEN converted THEN 1 ELSE 0 END) * 100.0 / COUNT(*), \n          2\n        ) as conversion_rate\n      FROM ab_test_results\n      GROUP BY test_group\n      ORDER BY conversion_rate DESC;\n    	Advanced A/B Testing problem with real business applications.	2025-08-11 09:51:10.028412	[["treatment", "5", "4", "80.00"], ["control", "5", "2", "40.00"]]
9c0baea2-8250-4bd7-9161-eccc5dcaca8e	5b8ca9e4-c845-416a-bb0b-6429619dcbcc	postgresql	\n      CREATE TABLE solar_installations (\n          installation_id INT,\n          location VARCHAR(50),\n          panel_capacity_kw DECIMAL(8,2),\n          daily_production_kwh DECIMAL(10,2),\n          installation_date DATE\n      );\n      \n      INSERT INTO solar_installations VALUES \n      (1, 'California', 25.0, 120.5, '2023-06-15'),\n      (2, 'Arizona', 30.0, 135.2, '2023-07-20'),\n      (3, 'Nevada', 20.0, 85.8, '2023-08-10'),\n      (4, 'Texas', 35.0, 145.6, '2023-05-25'),\n      (5, 'Florida', 28.0, 98.4, '2023-09-12'),\n      (6, 'Colorado', 22.0, 102.3, '2023-07-08');\n    	\n      WITH efficiency_metrics AS (\n        SELECT \n          installation_id,\n          location,\n          daily_production_kwh as avg_daily_production,\n          ROUND(daily_production_kwh / panel_capacity_kw, 2) as efficiency_rating,\n          AVG(daily_production_kwh / panel_capacity_kw) OVER() as overall_avg_efficiency\n        FROM solar_installations\n      )\n      SELECT \n        installation_id,\n        avg_daily_production,\n        efficiency_rating,\n        CASE \n          WHEN efficiency_rating >= overall_avg_efficiency THEN 'Above Average'\n          ELSE 'Below Average'\n        END as performance_category\n      FROM efficiency_metrics\n      WHERE efficiency_rating < overall_avg_efficiency\n      ORDER BY efficiency_rating;\n    	Advanced Energy Analytics problem with real business applications.	2025-08-11 09:51:10.032285	[["5", "98.40", "3.51", "Below Average"], ["4", "145.60", "4.16", "Below Average"], ["3", "85.80", "4.29", "Below Average"]]
8da05a7e-af86-4af9-ba97-b454cc65c3b1	5c8c0ec6-3d89-4c9d-b360-e449a870df7d	postgresql	CREATE TABLE nike_sales (\n        sale_id INT PRIMARY KEY,\n        product_line VARCHAR(50),\n        region VARCHAR(30),\n        sales_amount DECIMAL(10,2),\n        units_sold INT,\n        sale_date DATE\n    );\n    \n    INSERT INTO nike_sales VALUES \n    (1, 'Air Jordan', 'North America', 125000.50, 450, '2024-01-15'),\n    (2, 'Air Max', 'Europe', 89000.75, 320, '2024-01-15'),\n    (3, 'React Running', 'Asia Pacific', 67000.25, 280, '2024-01-16'),\n    (4, 'Air Force 1', 'North America', 98000.80, 380, '2024-01-16'),\n    (5, 'Dunk Series', 'Europe', 78000.60, 295, '2024-01-17'),\n    (6, 'Air Jordan', 'Asia Pacific', 156000.25, 520, '2024-01-17'),\n    (7, 'Training Shoes', 'Latin America', 45000.90, 215, '2024-01-18'),\n    (8, 'Air Max', 'North America', 134000.40, 475, '2024-01-18');	SELECT region, SUM(sales_amount) as total_sales\nFROM nike_sales \nGROUP BY region \nHAVING SUM(sales_amount) > 500000 \nORDER BY total_sales DESC;	Regional sales analysis with GROUP BY and HAVING clause for identifying high-performing markets.	2025-08-11 09:51:12.112684	\N
00cc1e76-7a72-45af-a110-7c079826b812	f8b1fc39-54aa-469b-ad12-af368903bb47	postgresql	CREATE TABLE disney_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO disney_revenue VALUES (1, 'Core Business', 1382655.50, 'Q1 2024'), (2, 'Digital Services', 255399.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM disney_revenue GROUP BY segment ORDER BY total_revenue DESC;	Aggregation analysis for Fortune 100 companies	2025-08-11 13:08:07.774661	\N
57d97e4c-2cbb-4280-bb4a-5f78dd0e43d0	387f6300-3716-40e2-85dc-a792cb32a6b1	postgresql	CREATE TABLE merck_pipeline (\n        project_id INT PRIMARY KEY,\n        drug_candidate VARCHAR(100),\n        therapeutic_area VARCHAR(50),\n        development_phase VARCHAR(30),\n        status VARCHAR(20),\n        estimated_completion DATE\n    );\n    \n    INSERT INTO merck_pipeline VALUES \n    (1, 'MK-7845 Oncology', 'Oncology', 'Phase III', 'Active', '2025-06-15'),\n    (2, 'MK-2190 Diabetes', 'Endocrinology', 'Phase II', 'Active', '2025-09-20'),\n    (3, 'MK-5672 Cardio', 'Cardiovascular', 'Phase I', 'Active', '2026-03-10'),\n    (4, 'MK-8834 Immuno', 'Immunology', 'Phase II', 'On Hold', '2025-12-25'),\n    (5, 'MK-3451 Neuro', 'Neurology', 'Phase I', 'Active', '2026-08-30'),\n    (6, 'MK-9012 Cancer', 'Oncology', 'Phase II', 'Active', '2025-11-15'),\n    (7, 'MK-6789 Arthritis', 'Rheumatology', 'Phase III', 'Active', '2024-10-05'),\n    (8, 'MK-4567 Migraine', 'Neurology', 'Phase I', 'Active', '2026-07-18');	SELECT therapeutic_area, COUNT(*) as active_projects\nFROM merck_pipeline \nWHERE status = 'Active'\nGROUP BY therapeutic_area \nHAVING COUNT(*) > 5 \nORDER BY active_projects DESC;	Pharmaceutical pipeline analysis using WHERE clause with GROUP BY and HAVING for active project filtering.	2025-08-11 09:51:12.114915	\N
61818094-7930-4aea-81ce-e7872e2d2375	ebd41598-6542-40d8-9b54-c7b010de4ca5	postgresql	CREATE TABLE citibank_transactions (\n        transaction_id INT PRIMARY KEY,\n        card_number VARCHAR(20),\n        merchant_category VARCHAR(50),\n        transaction_amount DECIMAL(10,2),\n        transaction_date DATE,\n        merchant_location VARCHAR(100)\n    );\n    \n    INSERT INTO citibank_transactions VALUES \n    (1, '4532-****-****-1234', 'Gas Stations', 45.75, '2024-01-15', 'New York, NY'),\n    (2, '4532-****-****-1234', 'Restaurants', 78.50, '2024-01-16', 'New York, NY'),\n    (3, '4532-****-****-1234', 'Department Stores', 245.00, '2024-01-17', 'New York, NY'),\n    (4, '4532-****-****-1234', 'Electronics', 1899.99, '2024-01-18', 'New York, NY'),\n    (5, '4532-****-****-1234', 'Grocery Stores', 125.30, '2024-01-19', 'New York, NY'),\n    (6, '4111-****-****-5678', 'Gas Stations', 52.25, '2024-01-15', 'Los Angeles, CA'),\n    (7, '4111-****-****-5678', 'Restaurants', 95.75, '2024-01-16', 'Los Angeles, CA'),\n    (8, '4111-****-****-5678', 'Hotels', 450.00, '2024-01-17', 'Las Vegas, NV'),\n    (9, '4111-****-****-5678', 'Airlines', 850.00, '2024-01-18', 'Las Vegas, NV'),\n    (10, '4111-****-****-5678', 'Electronics', 2500.00, '2024-01-19', 'Las Vegas, NV');	WITH rolling_averages AS (\n        SELECT \n            transaction_id,\n            card_number,\n            merchant_category,\n            transaction_amount,\n            transaction_date,\n            merchant_location,\n            AVG(transaction_amount) OVER (\n                PARTITION BY card_number \n                ORDER BY transaction_date \n                ROWS BETWEEN 6 PRECEDING AND 1 PRECEDING\n            ) as rolling_7day_avg\n        FROM citibank_transactions\n    )\n    SELECT \n        card_number,\n        merchant_category,\n        transaction_amount,\n        ROUND(rolling_7day_avg, 2) as rolling_average,\n        ROUND(transaction_amount / NULLIF(rolling_7day_avg, 0), 2) as deviation_multiple,\n        transaction_date,\n        merchant_location\n    FROM rolling_averages\n    WHERE transaction_amount > (3 * rolling_7day_avg)\n        AND rolling_7day_avg IS NOT NULL\n    ORDER BY deviation_multiple DESC;	Advanced fraud detection using window functions for rolling averages and transaction anomaly identification based on spending velocity patterns.	2025-08-11 09:51:12.115861	\N
302b245c-a441-4e6f-b2d2-2b2273f2da9c	71d86caa-b97f-448e-8282-93dc930abe4b	postgresql	CREATE TABLE lm_government_agencies (\n        agency_id INT PRIMARY KEY,\n        agency_name VARCHAR(100),\n        agency_type VARCHAR(50),\n        primary_contact VARCHAR(100)\n    );\n    \n    CREATE TABLE lm_defense_contracts (\n        contract_id INT PRIMARY KEY,\n        agency_id INT,\n        contract_name VARCHAR(150),\n        contract_value DECIMAL(15,2),\n        contract_type VARCHAR(50),\n        start_date DATE,\n        end_date DATE,\n        status VARCHAR(30)\n    );\n    \n    INSERT INTO lm_government_agencies VALUES \n    (101, 'U.S. Air Force', 'Military', 'Colonel Sarah Johnson'),\n    (102, 'U.S. Navy', 'Military', 'Admiral Michael Chen'),\n    (103, 'NASA', 'Space Agency', 'Director Emily Rodriguez'),\n    (104, 'Department of Defense', 'Defense', 'General David Kim');\n    \n    INSERT INTO lm_defense_contracts VALUES \n    (1, 101, 'F-35 Lightning II Program', 15000000000.00, 'Aircraft Development', '2020-01-15', '2025-12-31', 'Active'),\n    (2, 102, 'Aegis Combat System', 8500000000.00, 'Naval Defense', '2019-06-01', '2024-05-30', 'Active'),\n    (3, 103, 'Orion Spacecraft', 12000000000.00, 'Space Exploration', '2018-03-15', '2026-09-15', 'Active'),\n    (4, 104, 'THAAD Missile Defense', 7200000000.00, 'Missile Defense', '2019-09-01', '2024-08-31', 'Active'),\n    (5, 101, 'C-130J Super Hercules', 6800000000.00, 'Transport Aircraft', '2020-04-01', '2025-03-31', 'Active'),\n    (6, 102, 'Littoral Combat Ship', 5500000000.00, 'Naval Vessels', '2018-11-15', '2023-10-15', 'Completed');	SELECT \n        a.agency_name,\n        a.agency_type,\n        COUNT(c.contract_id) as total_contracts,\n        SUM(c.contract_value) / 1000000000 as total_value_billions,\n        AVG(EXTRACT(YEAR FROM AGE(c.end_date, c.start_date))) as avg_duration_years\n    FROM lm_government_agencies a\n    JOIN lm_defense_contracts c ON a.agency_id = c.agency_id\n    GROUP BY a.agency_id, a.agency_name, a.agency_type\n    HAVING SUM(c.contract_value) > 1000000000\n    ORDER BY SUM(c.contract_value) DESC;	Defense contracting analysis using JOINs with complex aggregation and date calculations for government agency relationship management.	2025-08-11 09:51:12.116791	\N
bcface37-02ba-4bd6-8212-a482a1fa5f51	9992b85b-7f74-4ada-ab73-2daa3ef60e32	postgresql	CREATE TABLE wf_borrowers (\n        borrower_id INT PRIMARY KEY,\n        credit_score INT,\n        annual_income DECIMAL(12,2),\n        employment_years DECIMAL(4,2),\n        total_debt DECIMAL(12,2),\n        loan_amount DECIMAL(12,2),\n        home_value DECIMAL(12,2),\n        loan_status VARCHAR(20)\n    );\n    \n    INSERT INTO wf_borrowers VALUES \n    (9001, 750, 95000.00, 8.5, 25000.00, 380000.00, 450000.00, 'Current'),\n    (9002, 680, 72000.00, 4.2, 35000.00, 320000.00, 400000.00, 'Default'),\n    (9003, 720, 88000.00, 6.8, 28000.00, 350000.00, 425000.00, 'Current'),\n    (9004, 620, 55000.00, 2.1, 45000.00, 280000.00, 325000.00, 'Default'),\n    (9005, 780, 125000.00, 12.3, 18000.00, 420000.00, 500000.00, 'Current'),\n    (9006, 640, 58000.00, 3.5, 42000.00, 295000.00, 340000.00, 'Default'),\n    (9007, 710, 82000.00, 7.2, 31000.00, 365000.00, 430000.00, 'Current'),\n    (9008, 590, 48000.00, 1.8, 38000.00, 265000.00, 310000.00, 'Default'),\n    (9009, 800, 150000.00, 15.6, 22000.00, 480000.00, 580000.00, 'Current'),\n    (9010, 660, 68000.00, 5.1, 39000.00, 315000.00, 375000.00, 'Current');	WITH risk_calculations AS (\n        SELECT \n            borrower_id,\n            credit_score,\n            annual_income,\n            employment_years,\n            loan_amount,\n            home_value,\n            total_debt,\n            loan_status,\n            (total_debt / annual_income) as debt_to_income_ratio,\n            (loan_amount / home_value) as loan_to_value_ratio,\n            -- Normalize factors to 0-100 scale and apply weights\n            (\n                (credit_score / 850.0 * 100 * 0.30) +\n                ((1 - LEAST(total_debt / annual_income, 1)) * 100 * 0.25) +\n                ((1 - LEAST(loan_amount / home_value, 1)) * 100 * 0.25) +\n                (LEAST(employment_years / 20.0, 1) * 100 * 0.20)\n            ) as composite_risk_score\n        FROM wf_borrowers\n    ),\n    risk_deciles AS (\n        SELECT \n            *,\n            NTILE(10) OVER (ORDER BY composite_risk_score) as risk_decile\n        FROM risk_calculations\n    ),\n    decile_analysis AS (\n        SELECT \n            risk_decile,\n            COUNT(*) as borrower_count,\n            AVG(composite_risk_score) as avg_risk_score,\n            SUM(CASE WHEN loan_status = 'Default' THEN 1 ELSE 0 END) as defaults,\n            AVG(loan_amount) as avg_loan_amount,\n            CAST(SUM(CASE WHEN loan_status = 'Default' THEN 1 ELSE 0 END) AS DECIMAL) / COUNT(*) as default_rate\n        FROM risk_deciles\n        GROUP BY risk_decile\n    )\n    SELECT \n        risk_decile,\n        borrower_count,\n        ROUND(avg_risk_score, 2) as avg_risk_score,\n        defaults,\n        ROUND(default_rate * 100, 2) as default_rate_percent,\n        ROUND(avg_loan_amount, 0) as avg_loan_amount,\n        ROUND(default_rate * avg_loan_amount * 0.5, 0) as expected_loss_per_borrower\n    FROM decile_analysis\n    WHERE default_rate > 0.08\n    ORDER BY risk_decile DESC;	Sophisticated mortgage risk modeling using weighted composite scoring, decile analysis, and expected loss calculations for regulatory compliance and risk management.	2025-08-11 09:51:12.118069	\N
0d5f368b-6e34-4b61-954b-b3f1a735f7c5	eb15df93-f7ca-4177-97d4-2ff06bc7c917	postgresql	CREATE TABLE mckinsey_clients (\n        client_id INT PRIMARY KEY,\n        client_name VARCHAR(100),\n        industry VARCHAR(50),\n        client_tier VARCHAR(20),\n        relationship_start DATE\n    );\n    \n    CREATE TABLE mckinsey_practices (\n        practice_id INT PRIMARY KEY,\n        practice_name VARCHAR(100),\n        practice_leader VARCHAR(100),\n        standard_hourly_rate DECIMAL(8,2)\n    );\n    \n    CREATE TABLE mckinsey_engagements (\n        engagement_id INT PRIMARY KEY,\n        client_id INT,\n        practice_id INT,\n        engagement_name VARCHAR(150),\n        total_revenue DECIMAL(12,2),\n        consultant_hours INT,\n        start_date DATE,\n        end_date DATE,\n        status VARCHAR(30)\n    );\n    \n    INSERT INTO mckinsey_clients VALUES \n    (701, 'Global Manufacturing Corp', 'Manufacturing', 'Tier 1', '2020-03-15'),\n    (702, 'Tech Innovation Inc', 'Technology', 'Tier 1', '2019-08-20'),\n    (703, 'Financial Services Group', 'Financial Services', 'Tier 2', '2021-01-10'),\n    (704, 'Healthcare Systems Ltd', 'Healthcare', 'Tier 1', '2018-11-05');\n    \n    INSERT INTO mckinsey_practices VALUES \n    (201, 'Strategy & Corporate Finance', 'Jane Mitchell', 850.00),\n    (202, 'Digital & Technology', 'Robert Chen', 950.00),\n    (203, 'Operations', 'Sarah Johnson', 750.00),\n    (204, 'Risk & Compliance', 'Michael Rodriguez', 800.00);\n    \n    INSERT INTO mckinsey_engagements VALUES \n    (1, 701, 201, 'Digital Transformation Strategy', 4500000.00, 3200, '2024-01-15', '2024-06-15', 'Active'),\n    (2, 702, 202, 'AI Implementation Roadmap', 3200000.00, 2800, '2024-02-01', '2024-07-01', 'Active'),\n    (3, 703, 204, 'Regulatory Compliance Program', 2800000.00, 2400, '2024-01-10', '2024-08-10', 'Active'),\n    (4, 704, 203, 'Supply Chain Optimization', 3800000.00, 3600, '2023-11-01', '2024-05-01', 'Completed'),\n    (5, 701, 202, 'Technology Integration', 2500000.00, 2200, '2024-03-01', '2024-08-01', 'Active'),\n    (6, 702, 201, 'Market Entry Strategy', 2200000.00, 1800, '2024-01-20', '2024-06-20', 'Active');	WITH engagement_economics AS (\n        SELECT \n            c.client_name,\n            c.industry,\n            c.client_tier,\n            p.practice_name,\n            p.practice_leader,\n            e.engagement_name,\n            e.total_revenue,\n            e.consultant_hours,\n            p.standard_hourly_rate,\n            -- Calculate total costs with overhead\n            (e.consultant_hours * p.standard_hourly_rate * 1.4) as total_costs,\n            -- Calculate profit\n            (e.total_revenue - (e.consultant_hours * p.standard_hourly_rate * 1.4)) as gross_profit,\n            e.start_date,\n            e.end_date\n        FROM mckinsey_clients c\n        JOIN mckinsey_engagements e ON c.client_id = e.client_id\n        JOIN mckinsey_practices p ON e.practice_id = p.practice_id\n        WHERE e.status IN ('Active', 'Completed')\n    ),\n    client_practice_summary AS (\n        SELECT \n            client_name,\n            industry,\n            practice_name,\n            COUNT(*) as engagement_count,\n            SUM(total_revenue) as annual_revenue,\n            SUM(total_costs) as annual_costs,\n            SUM(gross_profit) as annual_profit,\n            AVG(consultant_hours) as avg_engagement_hours,\n            SUM(consultant_hours) as total_consultant_hours\n        FROM engagement_economics\n        WHERE start_date >= CURRENT_DATE - INTERVAL '12 months'\n        GROUP BY client_name, industry, practice_name\n    )\n    SELECT \n        client_name,\n        industry,\n        practice_name,\n        engagement_count,\n        ROUND(annual_revenue / 1000000, 2) as annual_revenue_millions,\n        ROUND(annual_costs / 1000000, 2) as annual_costs_millions,\n        ROUND(annual_profit / 1000000, 2) as annual_profit_millions,\n        ROUND((annual_profit / annual_revenue) * 100, 2) as profit_margin_percent,\n        ROUND(avg_engagement_hours, 0) as avg_hours_per_engagement,\n        total_consultant_hours as total_utilization_hours\n    FROM client_practice_summary\n    WHERE annual_revenue > 2000000\n        AND (annual_profit / annual_revenue) > 0.25\n    ORDER BY profit_margin_percent DESC;	Management consulting profitability analysis using complex cost modeling, overhead calculations, and client-practice performance optimization for strategic relationship management.	2025-08-11 09:51:12.119238	\N
eccbc576-53c0-473c-b36a-376fc3e5eb2e	f5c2c06b-023a-4e07-9240-18f97fefcde2	postgresql	CREATE TABLE netflix_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO netflix_revenue VALUES (1, 'Core Business', 1126916.50, 'Q1 2024'), (2, 'Digital Services', 351942.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM netflix_revenue GROUP BY segment ORDER BY total_revenue DESC;	Basic Queries analysis for Fortune 100 companies	2025-08-11 13:08:07.776199	\N
6238f17d-9a5c-4a45-ac0c-5d64a7372ad0	15382d01-697d-44bc-bfaa-931fec0cbdae	postgresql	CREATE TABLE uber_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO uber_revenue VALUES (1, 'Core Business', 874494.50, 'Q1 2024'), (2, 'Digital Services', 410617.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM uber_revenue GROUP BY segment ORDER BY total_revenue DESC;	Aggregation analysis for Fortune 100 companies	2025-08-11 13:08:07.817558	\N
a3481167-196c-4779-bd51-d18dc05a92fa	77b5ad49-9a65-4c07-8d89-3a98b9c6040c	postgresql	CREATE TABLE verizon_towers (\n        tower_id INT PRIMARY KEY,\n        region VARCHAR(30),\n        tower_type VARCHAR(20),\n        signal_strength INT,\n        coverage_radius_miles DECIMAL(5,2),\n        installation_date DATE\n    );\n    \n    INSERT INTO verizon_towers VALUES \n    (1001, 'Northeast', '4G LTE', 85, 3.5, '2023-06-15'),\n    (1002, 'Southeast', '5G', 92, 2.8, '2023-08-20'),\n    (1003, 'Midwest', '4G LTE', 78, 4.2, '2023-05-10'),\n    (1004, 'West Coast', '5G', 88, 3.1, '2023-09-05'),\n    (1005, 'Northeast', '5G', 95, 2.5, '2023-07-12'),\n    (1006, 'Southwest', '4G LTE', 82, 3.8, '2023-04-18'),\n    (1007, 'Southeast', '4G LTE', 80, 4.0, '2023-06-25'),\n    (1008, 'West Coast', '5G', 90, 2.9, '2023-10-08');	SELECT region, COUNT(*) as tower_count\nFROM verizon_towers \nGROUP BY region \nHAVING COUNT(*) > 100 \nORDER BY tower_count DESC;	Telecommunications infrastructure analysis using GROUP BY and HAVING clause for network coverage evaluation.	2025-08-11 09:51:15.98801	\N
68e07a6b-8c5d-4fd8-987d-109cef9dc5a8	46126968-4455-4602-821b-06375609254f	postgresql	CREATE TABLE att_service_calls (\n        call_id INT PRIMARY KEY,\n        service_category VARCHAR(50),\n        call_duration_minutes INT,\n        customer_satisfaction INT,\n        resolution_status VARCHAR(20),\n        call_date DATE\n    );\n    \n    INSERT INTO att_service_calls VALUES \n    (1, 'Billing Support', 12, 4, 'Resolved', '2024-01-15'),\n    (2, 'Technical Support', 25, 3, 'Escalated', '2024-01-15'),\n    (3, 'Account Management', 8, 5, 'Resolved', '2024-01-16'),\n    (4, 'Device Support', 18, 4, 'Resolved', '2024-01-16'),\n    (5, 'Billing Support', 15, 2, 'Unresolved', '2024-01-17'),\n    (6, 'Network Issues', 30, 3, 'Escalated', '2024-01-17'),\n    (7, 'Technical Support', 22, 4, 'Resolved', '2024-01-18'),\n    (8, 'Account Management', 6, 5, 'Resolved', '2024-01-18');	SELECT service_category, COUNT(*) as total_calls\nFROM att_service_calls \nGROUP BY service_category \nHAVING COUNT(*) > 1000 \nORDER BY total_calls DESC;	Customer service analytics with GROUP BY and HAVING for call volume analysis across service categories.	2025-08-11 09:51:15.990768	\N
fdf8a0fe-aa02-4723-880f-d9d35706e235	58a7cb0d-634f-408b-a5b0-36c0599c2b50	postgresql	CREATE TABLE ford_sales (\n        sale_id INT PRIMARY KEY,\n        vehicle_category VARCHAR(30),\n        model_name VARCHAR(50),\n        units_sold INT,\n        sale_region VARCHAR(30),\n        sale_date DATE\n    );\n    \n    INSERT INTO ford_sales VALUES \n    (1, 'Trucks', 'F-150', 2500, 'North America', '2024-01-15'),\n    (2, 'SUVs', 'Explorer', 1800, 'North America', '2024-01-15'),\n    (3, 'Sedans', 'Fusion', 950, 'Europe', '2024-01-16'),\n    (4, 'Electric', 'Mustang Mach-E', 1200, 'North America', '2024-01-16'),\n    (5, 'Trucks', 'Ranger', 1400, 'Asia Pacific', '2024-01-17'),\n    (6, 'SUVs', 'Escape', 1600, 'Europe', '2024-01-17'),\n    (7, 'Electric', 'F-150 Lightning', 800, 'North America', '2024-01-18'),\n    (8, 'Sedans', 'Mustang', 1100, 'Global', '2024-01-18');	SELECT vehicle_category, SUM(units_sold) as total_units\nFROM ford_sales \nGROUP BY vehicle_category \nHAVING SUM(units_sold) > 10000 \nORDER BY total_units DESC;	Automotive sales analysis using SUM aggregation with HAVING clause for production planning insights.	2025-08-11 09:51:15.992549	\N
fd1176ce-509e-4258-b5e6-aa95ee8817f8	02b4dc7d-b5e4-44c5-8340-93a0d4414680	postgresql	CREATE TABLE jpmorgan_trading (\n        trade_id INT PRIMARY KEY,\n        asset_class VARCHAR(30),\n        trading_strategy VARCHAR(50),\n        daily_pnl DECIMAL(12,2),\n        trader_id INT,\n        trade_date DATE\n    );\n    \n    INSERT INTO jpmorgan_trading VALUES \n    (1, 'Equities', 'Momentum Trading', 125000.50, 201, '2024-01-15'),\n    (2, 'Fixed Income', 'Yield Curve Trading', 85000.25, 202, '2024-01-15'),\n    (3, 'Currencies', 'FX Carry Trade', 67000.75, 203, '2024-01-15'),\n    (4, 'Equities', 'Statistical Arbitrage', 98000.80, 204, '2024-01-16'),\n    (5, 'Commodities', 'Energy Trading', 156000.60, 205, '2024-01-16'),\n    (6, 'Fixed Income', 'Credit Spreads', 78000.45, 206, '2024-01-16'),\n    (7, 'Equities', 'Momentum Trading', 142000.90, 201, '2024-01-17'),\n    (8, 'Currencies', 'FX Momentum', 89000.25, 207, '2024-01-17'),\n    (9, 'Fixed Income', 'Yield Curve Trading', 92000.15, 202, '2024-01-18'),\n    (10, 'Commodities', 'Metals Trading', 134000.80, 208, '2024-01-18');	WITH strategy_performance AS (\n        SELECT \n            asset_class,\n            trading_strategy,\n            AVG(daily_pnl) as avg_daily_pnl,\n            COUNT(*) as trading_days,\n            SUM(daily_pnl) as total_pnl\n        FROM jpmorgan_trading\n        GROUP BY asset_class, trading_strategy\n    ),\n    ranked_strategies AS (\n        SELECT \n            *,\n            ROW_NUMBER() OVER (PARTITION BY asset_class ORDER BY avg_daily_pnl DESC) as performance_rank\n        FROM strategy_performance\n    )\n    SELECT \n        asset_class,\n        trading_strategy,\n        performance_rank,\n        ROUND(avg_daily_pnl, 2) as avg_daily_pnl,\n        trading_days,\n        ROUND(total_pnl, 2) as total_pnl\n    FROM ranked_strategies\n    WHERE performance_rank <= 2\n    ORDER BY asset_class, performance_rank;	Investment banking performance analysis using window functions with ROW_NUMBER() for ranking trading strategies within asset classes.	2025-08-11 09:51:15.993926	\N
170623e2-c78e-47e9-834c-715b73d33821	7770d8bd-25ce-4caf-908f-6371e1a43e3c	postgresql	CREATE TABLE cvs_pharmacies (\n        pharmacy_id INT PRIMARY KEY,\n        store_location VARCHAR(100),\n        region VARCHAR(30),\n        manager_name VARCHAR(100)\n    );\n    \n    CREATE TABLE cvs_medication_inventory (\n        inventory_id INT PRIMARY KEY,\n        pharmacy_id INT,\n        medication_name VARCHAR(100),\n        therapeutic_category VARCHAR(50),\n        current_stock INT,\n        daily_usage DECIMAL(6,2),\n        last_restock_date DATE\n    );\n    \n    INSERT INTO cvs_pharmacies VALUES \n    (5001, 'Downtown Chicago', 'Midwest', 'Sarah Johnson'),\n    (5002, 'Beverly Hills', 'West Coast', 'Michael Chen'),\n    (5003, 'Manhattan NYC', 'Northeast', 'Emily Rodriguez'),\n    (5004, 'Miami Beach', 'Southeast', 'David Kim');\n    \n    INSERT INTO cvs_medication_inventory VALUES \n    (1, 5001, 'Lisinopril', 'Cardiovascular', 180, 6.5, '2024-01-10'),\n    (2, 5001, 'Metformin', 'Diabetes', 95, 4.2, '2024-01-12'),\n    (3, 5001, 'Atorvastatin', 'Cholesterol', 120, 5.8, '2024-01-08'),\n    (4, 5001, 'Sertraline', 'Mental Health', 85, 3.1, '2024-01-15'),\n    (5, 5002, 'Omeprazole', 'Gastrointestinal', 200, 7.2, '2024-01-11'),\n    (6, 5002, 'Amlodipine', 'Cardiovascular', 110, 4.8, '2024-01-13'),\n    (7, 5003, 'Levothyroxine', 'Endocrine', 150, 5.5, '2024-01-09'),\n    (8, 5003, 'Gabapentin', 'Neurological', 75, 2.9, '2024-01-14'),\n    (9, 5004, 'Prednisone', 'Inflammatory', 90, 3.6, '2024-01-12'),\n    (10, 5004, 'Albuterol', 'Respiratory', 165, 6.1, '2024-01-10');	WITH inventory_analysis AS (\n        SELECT \n            i.pharmacy_id,\n            i.therapeutic_category,\n            AVG(i.current_stock / i.daily_usage) as avg_days_supply\n        FROM cvs_medication_inventory i\n        GROUP BY i.pharmacy_id, i.therapeutic_category\n    ),\n    low_inventory_categories AS (\n        SELECT \n            pharmacy_id,\n            COUNT(DISTINCT therapeutic_category) as low_inventory_categories,\n            AVG(avg_days_supply) as overall_avg_days_supply\n        FROM inventory_analysis\n        WHERE avg_days_supply < 30\n        GROUP BY pharmacy_id\n    )\n    SELECT \n        p.pharmacy_id,\n        p.store_location,\n        p.region,\n        p.manager_name,\n        lic.low_inventory_categories,\n        ROUND(lic.overall_avg_days_supply, 1) as avg_days_supply\n    FROM cvs_pharmacies p\n    JOIN low_inventory_categories lic ON p.pharmacy_id = lic.pharmacy_id\n    WHERE lic.low_inventory_categories > 3\n    ORDER BY lic.low_inventory_categories DESC;	Healthcare supply chain analysis using CTEs and JOINs to identify multi-category inventory shortages across pharmacy locations.	2025-08-11 09:51:15.995636	\N
178168d7-d495-4756-8fc2-71cf018d01cb	addf9669-6c47-4410-8b73-354e852ff005	postgresql	CREATE TABLE jpmorgan_derivatives (instrument_id INT, derivative_type VARCHAR(50), notional_amount DECIMAL(15,2), market_value DECIMAL(15,2), delta DECIMAL(8,6), gamma DECIMAL(8,6), vega DECIMAL(8,6), theta DECIMAL(8,6));\n    INSERT INTO jpmorgan_derivatives VALUES (1, 'Interest Rate Swap', 100000000.00, 1250000.00, 0.045600, 0.000125, 0.008900, -450.75), (2, 'Credit Default Swap', 50000000.00, -125000.00, -0.012500, 0.000045, 0.005600, -125.25);	WITH risk_metrics AS (SELECT derivative_type, AVG(ABS(delta)) as avg_delta, AVG(ABS(gamma)) as avg_gamma, SUM(ABS(market_value)) as total_exposure FROM jpmorgan_derivatives GROUP BY derivative_type) SELECT derivative_type, ROUND(avg_delta, 6) as average_delta_risk, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for JPMorgan Derivatives Risk Analytics	2025-08-11 13:06:42.249918	\N
6e751fa2-a14e-49bd-86bd-c0694acbd19c	41facc79-e158-4b9f-a70d-05908d560d3e	postgresql	CREATE TABLE citadel_positions (\n        position_id INT PRIMARY KEY,\n        asset_name VARCHAR(100),\n        asset_class VARCHAR(30),\n        geographic_region VARCHAR(30),\n        current_weight DECIMAL(8,4),\n        expected_return DECIMAL(8,4),\n        volatility DECIMAL(8,4),\n        beta_to_portfolio DECIMAL(6,4)\n    );\n    \n    CREATE TABLE citadel_correlations (\n        asset1_id INT,\n        asset2_id INT,\n        correlation_coefficient DECIMAL(6,4),\n        observation_period VARCHAR(20)\n    );\n    \n    INSERT INTO citadel_positions VALUES \n    (1, 'S&P 500 Futures', 'Equities', 'North America', 0.2500, 0.0850, 0.1820, 1.0000),\n    (2, 'FTSE 100 Futures', 'Equities', 'Europe', 0.1500, 0.0720, 0.1950, 0.8500),\n    (3, '10Y Treasury Futures', 'Fixed Income', 'North America', 0.3000, 0.0320, 0.0850, -0.2500),\n    (4, 'German Bund Futures', 'Fixed Income', 'Europe', 0.1200, 0.0280, 0.0920, -0.1800),\n    (5, 'Gold Futures', 'Commodities', 'Global', 0.0800, 0.0450, 0.2200, 0.1200),\n    (6, 'EUR/USD Currency', 'Currencies', 'Global', 0.0500, 0.0150, 0.1100, 0.0800),\n    (7, 'Emerging Markets ETF', 'Equities', 'Asia Pacific', 0.0500, 0.0920, 0.2800, 1.2500);\n    \n    INSERT INTO citadel_correlations VALUES \n    (1, 2, 0.7500, 'Last 252 Days'),\n    (1, 3, -0.4500, 'Last 252 Days'),\n    (1, 4, -0.3200, 'Last 252 Days'),\n    (1, 5, 0.1500, 'Last 252 Days'),\n    (1, 6, -0.1800, 'Last 252 Days'),\n    (1, 7, 0.8200, 'Last 252 Days'),\n    (2, 3, -0.3800, 'Last 252 Days'),\n    (2, 4, -0.5200, 'Last 252 Days');	WITH portfolio_metrics AS (\n        SELECT \n            0.1650 as portfolio_volatility, -- Calculated portfolio volatility\n            COUNT(*) as total_positions\n        FROM citadel_positions\n    ),\n    risk_contributions AS (\n        SELECT \n            p.position_id,\n            p.asset_name,\n            p.asset_class,\n            p.geographic_region,\n            p.current_weight,\n            p.beta_to_portfolio,\n            pm.portfolio_volatility,\n            -- Risk Contribution = Weight × Beta × Portfolio Volatility\n            (p.current_weight * p.beta_to_portfolio * pm.portfolio_volatility) as risk_contribution,\n            -- Marginal risk contribution\n            (p.beta_to_portfolio * pm.portfolio_volatility) as marginal_risk_contribution\n        FROM citadel_positions p\n        CROSS JOIN portfolio_metrics pm\n    ),\n    total_risk AS (\n        SELECT SUM(ABS(risk_contribution)) as total_portfolio_risk\n        FROM risk_contributions\n    ),\n    risk_analysis AS (\n        SELECT \n            rc.*,\n            tr.total_portfolio_risk,\n            (ABS(rc.risk_contribution) / tr.total_portfolio_risk) as risk_contribution_pct,\n            -- Optimal weight for equal risk contribution\n            (1.0 / COUNT(*) OVER()) / rc.marginal_risk_contribution as optimal_weight\n        FROM risk_contributions rc\n        CROSS JOIN total_risk tr\n    )\n    SELECT \n        asset_name,\n        asset_class,\n        geographic_region,\n        ROUND(current_weight * 100, 2) as current_weight_pct,\n        ROUND(risk_contribution_pct * 100, 2) as risk_contribution_pct,\n        ROUND(optimal_weight * 100, 2) as optimal_weight_pct,\n        ROUND((optimal_weight - current_weight) * 100, 2) as rebalancing_delta_pct,\n        CASE \n            WHEN optimal_weight > current_weight THEN 'Increase Position'\n            WHEN optimal_weight < current_weight THEN 'Decrease Position'\n            ELSE 'Maintain Position'\n        END as rebalancing_action\n    FROM risk_analysis\n    WHERE risk_contribution_pct > 0.05\n    ORDER BY risk_contribution_pct DESC;	Advanced quantitative finance using risk parity methodology, portfolio risk decomposition, and optimal weight calculations for hedge fund portfolio management.	2025-08-11 09:51:15.997289	\N
8ae0716d-ed55-4781-a1e2-0575f63cb000	3e5c2aaf-f454-41cf-8898-ac134e7309d4	postgresql	CREATE TABLE aig_policies (\n        policy_id VARCHAR(20) PRIMARY KEY,\n        policyholder_name VARCHAR(100),\n        policy_type VARCHAR(30),\n        annual_premium DECIMAL(10,2),\n        coverage_amount DECIMAL(12,2),\n        policy_start_date DATE\n    );\n    \n    CREATE TABLE aig_claims (\n        claim_id VARCHAR(20) PRIMARY KEY,\n        policy_id VARCHAR(20),\n        claim_amount DECIMAL(12,2),\n        claim_type VARCHAR(30),\n        claim_date DATE,\n        adjuster_id INT,\n        processing_time_days INT,\n        claim_status VARCHAR(20)\n    );\n    \n    INSERT INTO aig_policies VALUES \n    ('POL-001', 'ABC Manufacturing', 'Commercial Property', 125000.00, 5000000.00, '2023-01-15'),\n    ('POL-002', 'Smith Family Trust', 'Homeowners', 3500.00, 750000.00, '2023-03-20'),\n    ('POL-003', 'Global Logistics Inc', 'Commercial Auto', 85000.00, 2500000.00, '2023-02-10'),\n    ('POL-004', 'Johnson Residence', 'Homeowners', 2800.00, 650000.00, '2023-04-05'),\n    ('POL-005', 'Tech Startup LLC', 'General Liability', 15000.00, 1000000.00, '2023-05-12');\n    \n    INSERT INTO aig_claims VALUES \n    ('CLM-001', 'POL-001', 245000.00, 'Equipment Damage', '2024-01-15', 301, 45, 'Approved'),\n    ('CLM-002', 'POL-002', 15000.00, 'Water Damage', '2024-01-20', 302, 12, 'Approved'),\n    ('CLM-003', 'POL-003', 85000.00, 'Vehicle Collision', '2024-01-18', 303, 25, 'Under Review'),\n    ('CLM-004', 'POL-001', 180000.00, 'Fire Damage', '2024-02-10', 301, 38, 'Approved'),\n    ('CLM-005', 'POL-002', 8500.00, 'Theft', '2024-02-15', 304, 18, 'Approved'),\n    ('CLM-006', 'POL-004', 25000.00, 'Storm Damage', '2024-02-20', 302, 22, 'Under Review'),\n    ('CLM-007', 'POL-003', 125000.00, 'Vehicle Collision', '2024-03-05', 305, 55, 'Denied'),\n    ('CLM-008', 'POL-005', 45000.00, 'Property Damage', '2024-03-10', 303, 30, 'Approved');	WITH policy_averages AS (\n        SELECT \n            policy_id,\n            AVG(claim_amount) as avg_claim_amount,\n            COUNT(*) as claim_frequency,\n            STDDEV(claim_amount) as claim_amount_stddev\n        FROM aig_claims\n        GROUP BY policy_id\n    ),\n    adjuster_consistency AS (\n        SELECT \n            adjuster_id,\n            AVG(processing_time_days) as avg_processing_time,\n            STDDEV(processing_time_days) as processing_time_stddev,\n            COUNT(*) as claims_handled\n        FROM aig_claims\n        GROUP BY adjuster_id\n    ),\n    time_pattern_analysis AS (\n        SELECT \n            claim_id,\n            policy_id,\n            -- Weekend/holiday claims (higher risk)\n            CASE WHEN EXTRACT(DOW FROM claim_date) IN (0, 6) THEN 1.0 ELSE 0.0 END as weekend_claim,\n            -- Multiple claims in short period\n            COUNT(*) OVER (PARTITION BY policy_id ORDER BY claim_date ROWS BETWEEN 30 PRECEDING AND CURRENT ROW) as claims_30_days\n        FROM aig_claims\n    ),\n    fraud_indicators AS (\n        SELECT \n            c.claim_id,\n            c.policy_id,\n            c.claim_amount,\n            c.claim_type,\n            c.adjuster_id,\n            p.policyholder_name,\n            p.coverage_amount,\n            -- Amount anomaly score (0-1)\n            LEAST(1.0, c.claim_amount / NULLIF(pa.avg_claim_amount, 0) / 3.0) as amount_anomaly_score,\n            -- Frequency score (0-1)  \n            LEAST(1.0, pa.claim_frequency / 5.0) as frequency_score,\n            -- Time pattern score (0-1)\n            (tp.weekend_claim + LEAST(1.0, tp.claims_30_days / 3.0)) / 2.0 as time_pattern_score,\n            -- Adjuster inconsistency score (0-1)\n            LEAST(1.0, ABS(c.processing_time_days - ac.avg_processing_time) / NULLIF(ac.processing_time_stddev, 0) / 2.0) as adjuster_score\n        FROM aig_claims c\n        JOIN aig_policies p ON c.policy_id = p.policy_id\n        LEFT JOIN policy_averages pa ON c.policy_id = pa.policy_id\n        LEFT JOIN adjuster_consistency ac ON c.adjuster_id = ac.adjuster_id\n        LEFT JOIN time_pattern_analysis tp ON c.claim_id = tp.claim_id\n    ),\n    fraud_scores AS (\n        SELECT \n            *,\n            -- Weighted composite fraud score\n            (COALESCE(amount_anomaly_score, 0) * 0.30 + \n             COALESCE(frequency_score, 0) * 0.25 + \n             COALESCE(time_pattern_score, 0) * 0.25 + \n             COALESCE(adjuster_score, 0) * 0.20) as composite_fraud_score\n        FROM fraud_indicators\n    ),\n    percentile_thresholds AS (\n        SELECT PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY composite_fraud_score) as fraud_threshold_90th\n        FROM fraud_scores\n    )\n    SELECT \n        fs.claim_id,\n        fs.policyholder_name,\n        fs.claim_type,\n        ROUND(fs.claim_amount, 2) as claim_amount,\n        ROUND(fs.amount_anomaly_score, 3) as amount_risk,\n        ROUND(fs.frequency_score, 3) as frequency_risk,\n        ROUND(fs.time_pattern_score, 3) as time_pattern_risk,\n        ROUND(fs.adjuster_score, 3) as adjuster_risk,\n        ROUND(fs.composite_fraud_score, 3) as fraud_score,\n        CASE \n            WHEN fs.composite_fraud_score >= 0.8 THEN 'Critical Priority'\n            WHEN fs.composite_fraud_score >= 0.6 THEN 'High Priority'\n            WHEN fs.composite_fraud_score >= 0.4 THEN 'Medium Priority'\n            ELSE 'Low Priority'\n        END as investigation_priority\n    FROM fraud_scores fs\n    CROSS JOIN percentile_thresholds pt\n    WHERE fs.composite_fraud_score >= pt.fraud_threshold_90th\n    ORDER BY fs.composite_fraud_score DESC;	Insurance fraud detection using composite risk scoring, statistical anomaly detection, and percentile-based threshold analysis for investigation prioritization.	2025-08-11 09:51:15.999144	\N
313dc45a-1386-4e5e-bdd6-415760f098d6	2a77e25e-17bb-4f89-af67-8634e9ca2153	postgresql	CREATE TABLE ibm_watson_usage (\n        request_id INT PRIMARY KEY,\n        service_category VARCHAR(50),\n        api_endpoint VARCHAR(100),\n        request_count INT,\n        processing_time_ms INT,\n        usage_date DATE\n    );\n    \n    INSERT INTO ibm_watson_usage VALUES \n    (1, 'Natural Language Processing', 'sentiment-analysis', 450, 125, '2024-01-15'),\n    (2, 'Computer Vision', 'object-detection', 320, 280, '2024-01-15'),\n    (3, 'Speech Recognition', 'speech-to-text', 280, 190, '2024-01-16'),\n    (4, 'Language Translation', 'translate', 380, 95, '2024-01-16'),\n    (5, 'Natural Language Processing', 'entity-extraction', 520, 150, '2024-01-17'),\n    (6, 'Computer Vision', 'image-classification', 410, 220, '2024-01-17'),\n    (7, 'Machine Learning', 'predictive-modeling', 195, 340, '2024-01-18'),\n    (8, 'Speech Recognition', 'text-to-speech', 315, 165, '2024-01-18');	SELECT service_category, SUM(request_count) as total_requests\nFROM ibm_watson_usage \nGROUP BY service_category \nHAVING SUM(request_count) > 1000 \nORDER BY total_requests DESC;	AI service analytics using GROUP BY and HAVING to identify high-usage service categories for capacity planning.	2025-08-11 09:51:21.527962	\N
7d4a44ee-ede3-4c4b-815c-25ce071b52a5	e250723c-d9ff-4f9b-81ab-ec5e5f5fddd0	postgresql	CREATE TABLE target_sales (\n        sale_id INT PRIMARY KEY,\n        product_category VARCHAR(50),\n        store_location VARCHAR(50),\n        revenue_amount DECIMAL(12,2),\n        units_sold INT,\n        sale_date DATE\n    );\n    \n    INSERT INTO target_sales VALUES \n    (1, 'Electronics', 'Minneapolis Central', 145000.50, 285, '2024-01-15'),\n    (2, 'Clothing & Accessories', 'Chicago North', 89000.75, 425, '2024-01-15'),\n    (3, 'Home & Garden', 'Denver West', 112000.25, 185, '2024-01-16'),\n    (4, 'Beauty & Personal Care', 'Dallas South', 78000.80, 320, '2024-01-16'),\n    (5, 'Electronics', 'Phoenix East', 134000.60, 245, '2024-01-17'),\n    (6, 'Grocery & Food', 'Atlanta Downtown', 95000.45, 580, '2024-01-17'),\n    (7, 'Clothing & Accessories', 'Seattle Center', 156000.90, 485, '2024-01-18'),\n    (8, 'Home & Garden', 'Miami Beach', 125000.40, 195, '2024-01-18');	SELECT product_category, SUM(revenue_amount) as total_revenue\nFROM target_sales \nGROUP BY product_category \nHAVING SUM(revenue_amount) > 1000000 \nORDER BY total_revenue DESC;	Retail category performance analysis using SUM aggregation with revenue thresholds for merchandising strategy.	2025-08-11 09:51:21.531569	\N
c31ddc6e-15a7-460a-8e12-d6163f618341	20b87983-9952-4f6b-ad79-96cce189c2e2	postgresql	CREATE TABLE capitalone_borrowers (\n        borrower_id INT PRIMARY KEY,\n        annual_income DECIMAL(12,2),\n        total_credit_limit DECIMAL(10,2),\n        current_balance DECIMAL(10,2),\n        payment_history_score INT,\n        months_on_file INT,\n        risk_score INT\n    );\n    \n    INSERT INTO capitalone_borrowers VALUES \n    (8001, 45000.00, 12000.00, 8500.00, 720, 36, 680),\n    (8002, 75000.00, 25000.00, 15000.00, 780, 48, 720),\n    (8003, 120000.00, 45000.00, 12000.00, 820, 72, 780),\n    (8004, 55000.00, 18000.00, 16500.00, 650, 24, 620),\n    (8005, 95000.00, 35000.00, 28000.00, 740, 60, 690),\n    (8006, 38000.00, 8000.00, 7200.00, 690, 30, 640),\n    (8007, 150000.00, 60000.00, 18000.00, 850, 84, 820),\n    (8008, 62000.00, 22000.00, 19800.00, 670, 42, 650),\n    (8009, 85000.00, 30000.00, 21000.00, 760, 54, 710),\n    (8010, 42000.00, 10000.00, 9500.00, 700, 18, 660);	WITH income_segments AS (\n        SELECT \n            borrower_id,\n            annual_income,\n            total_credit_limit,\n            current_balance,\n            payment_history_score,\n            risk_score,\n            (current_balance / total_credit_limit) as utilization_ratio,\n            CASE \n                WHEN annual_income < 50000 THEN 'Low Income (<$50K)'\n                WHEN annual_income < 75000 THEN 'Mid Income ($50K-$75K)'\n                WHEN annual_income < 100000 THEN 'Upper Mid ($75K-$100K)'\n                ELSE 'High Income (>$100K)'\n            END as income_bracket\n        FROM capitalone_borrowers\n    ),\n    ranked_borrowers AS (\n        SELECT \n            *,\n            ROW_NUMBER() OVER (PARTITION BY income_bracket ORDER BY utilization_ratio DESC) as utilization_rank\n        FROM income_segments\n    )\n    SELECT \n        income_bracket,\n        borrower_id,\n        ROUND(annual_income, 0) as annual_income,\n        ROUND(utilization_ratio * 100, 2) as utilization_pct,\n        payment_history_score,\n        risk_score,\n        utilization_rank\n    FROM ranked_borrowers\n    WHERE utilization_rank <= 3\n    ORDER BY income_bracket, utilization_rank;	Credit risk analysis using income segmentation, utilization ratio calculations, and window functions for borrower ranking within risk categories.	2025-08-11 09:51:21.532714	\N
e10d33cb-a8dc-4069-9846-26e5f15929d7	c28cd914-6ca0-4082-9358-090d21a4d66c	postgresql	CREATE TABLE costco_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO costco_revenue VALUES (1, 'Core Business', 1039882.50, 'Q1 2024'), (2, 'Digital Services', 298086.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM costco_revenue GROUP BY segment ORDER BY total_revenue DESC;	Aggregation analysis for Fortune 100 companies	2025-08-11 13:08:07.749433	\N
35dde1f9-a12f-46ab-93dc-0f0b22da3449	9da41be0-656e-4919-8987-25e333bef4c6	postgresql	CREATE TABLE home depot_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO home depot_revenue VALUES (1, 'Core Business', 788280.50, 'Q1 2024'), (2, 'Digital Services', 503448.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM home depot_revenue GROUP BY segment ORDER BY total_revenue DESC;	Basic Queries analysis for Fortune 100 companies	2025-08-11 13:08:07.773443	\N
42d0c6e6-eff0-4231-bd2a-281ccf33bb9e	7102509c-bbd8-4706-99c5-23b599ec9de3	postgresql	CREATE TABLE unitedhealth_centers (\n        center_id INT PRIMARY KEY,\n        center_name VARCHAR(100),\n        region VARCHAR(30),\n        center_manager VARCHAR(100)\n    );\n    \n    CREATE TABLE unitedhealth_claims (\n        claim_id INT PRIMARY KEY,\n        center_id INT,\n        claim_type VARCHAR(50),\n        processing_time_days DECIMAL(4,2),\n        claim_amount DECIMAL(10,2),\n        submission_date DATE\n    );\n    \n    INSERT INTO unitedhealth_centers VALUES \n    (501, 'Atlanta Processing Center', 'Southeast', 'Sarah Johnson'),\n    (502, 'Phoenix Operations Hub', 'Southwest', 'Michael Chen'),\n    (503, 'Chicago Central Facility', 'Midwest', 'Emily Rodriguez'),\n    (504, 'Portland Regional Center', 'Northwest', 'David Kim');\n    \n    INSERT INTO unitedhealth_claims VALUES \n    (1, 501, 'Inpatient Hospital', 2.5, 25000.00, '2024-01-15'),\n    (2, 501, 'Outpatient Surgery', 1.8, 8500.00, '2024-01-15'),\n    (3, 501, 'Emergency Room', 1.2, 3200.00, '2024-01-16'),\n    (4, 501, 'Prescription Drugs', 0.8, 450.00, '2024-01-16'),\n    (5, 501, 'Physical Therapy', 1.5, 1200.00, '2024-01-17'),\n    (6, 501, 'Diagnostic Imaging', 2.1, 2800.00, '2024-01-17'),\n    (7, 502, 'Inpatient Hospital', 3.2, 32000.00, '2024-01-15'),\n    (8, 502, 'Outpatient Surgery', 2.8, 9200.00, '2024-01-15'),\n    (9, 503, 'Emergency Room', 1.5, 2900.00, '2024-01-16'),\n    (10, 503, 'Prescription Drugs', 1.0, 380.00, '2024-01-16');	WITH center_performance AS (\n        SELECT \n            c.center_id,\n            c.center_name,\n            c.region,\n            c.center_manager,\n            COUNT(DISTINCT cl.claim_type) as claim_types_handled,\n            AVG(cl.processing_time_days) as avg_processing_time,\n            COUNT(cl.claim_id) as total_claims_processed,\n            SUM(cl.claim_amount) as total_claim_value\n        FROM unitedhealth_centers c\n        JOIN unitedhealth_claims cl ON c.center_id = cl.center_id\n        GROUP BY c.center_id, c.center_name, c.region, c.center_manager\n    )\n    SELECT \n        center_name,\n        region,\n        center_manager,\n        claim_types_handled,\n        ROUND(avg_processing_time, 2) as avg_processing_days,\n        total_claims_processed,\n        ROUND(total_claim_value, 0) as total_claim_value\n    FROM center_performance\n    WHERE claim_types_handled > 5 \n        AND avg_processing_time < 3.0\n    ORDER BY avg_processing_time ASC;	Healthcare operations analysis using JOINs with multi-criteria filtering for processing center efficiency evaluation across claim type diversity and speed metrics.	2025-08-11 09:51:21.53502	\N
81636d45-a7a3-4257-b338-745c0ce1e007	555fe7bc-059f-4b65-aa4f-0e6a318265d8	postgresql	CREATE TABLE renaissance_strategies (\n        strategy_id INT PRIMARY KEY,\n        strategy_name VARCHAR(100),\n        strategy_type VARCHAR(50),\n        portfolio_manager VARCHAR(100),\n        inception_date DATE\n    );\n    \n    CREATE TABLE renaissance_returns (\n        return_id INT PRIMARY KEY,\n        strategy_id INT,\n        return_date DATE,\n        strategy_return DECIMAL(8,6),\n        market_return DECIMAL(8,6),\n        size_factor DECIMAL(8,6),\n        value_factor DECIMAL(8,6),\n        momentum_factor DECIMAL(8,6),\n        risk_free_rate DECIMAL(8,6),\n        market_regime VARCHAR(20)\n    );\n    \n    INSERT INTO renaissance_strategies VALUES \n    (1, 'Statistical Arbitrage Alpha', 'Equity Market Neutral', 'Dr. Sarah Quantson', '2020-01-15'),\n    (2, 'Cross-Asset Momentum', 'Multi-Asset', 'Prof. Michael Alphus', '2019-06-20'),\n    (3, 'Mean Reversion Capture', 'Fixed Income Relative Value', 'Dr. Emily Factorus', '2021-03-10'),\n    (4, 'Volatility Surface Arb', 'Options Strategies', 'Dr. David Greekius', '2020-09-05');\n    \n    INSERT INTO renaissance_returns VALUES \n    (1, 1, '2024-01-15', 0.0085, 0.0125, -0.0032, 0.0018, -0.0045, 0.0035, 'Bull Market'),\n    (2, 1, '2024-01-16', 0.0092, 0.0098, -0.0028, 0.0022, -0.0038, 0.0035, 'Bull Market'),\n    (3, 1, '2024-01-17', -0.0025, -0.0156, 0.0045, -0.0031, 0.0052, 0.0035, 'Bear Market'),\n    (4, 1, '2024-01-18', 0.0078, 0.0089, -0.0035, 0.0015, -0.0042, 0.0035, 'Bull Market'),\n    (5, 2, '2024-01-15', 0.0156, 0.0125, 0.0018, -0.0025, 0.0089, 0.0035, 'Bull Market'),\n    (6, 2, '2024-01-16', 0.0134, 0.0098, 0.0022, -0.0018, 0.0076, 0.0035, 'Bull Market'),\n    (7, 2, '2024-01-17', -0.0089, -0.0156, -0.0038, 0.0045, -0.0098, 0.0035, 'Bear Market'),\n    (8, 2, '2024-01-18', 0.0142, 0.0089, 0.0015, -0.0022, 0.0082, 0.0035, 'Bull Market'),\n    (9, 3, '2024-01-15', 0.0045, 0.0125, 0.0008, 0.0035, -0.0018, 0.0035, 'Bull Market'),\n    (10, 3, '2024-01-16', 0.0038, 0.0098, 0.0012, 0.0028, -0.0022, 0.0035, 'Bull Market');	WITH factor_regressions AS (\n        SELECT \n            s.strategy_id,\n            s.strategy_name,\n            s.strategy_type,\n            COUNT(r.return_id) as observations,\n            AVG(r.strategy_return - r.risk_free_rate) as avg_excess_return,\n            STDDEV(r.strategy_return - r.risk_free_rate) as return_volatility,\n            -- Multi-factor alpha calculation (simplified - in reality would use proper regression)\n            AVG(r.strategy_return - r.risk_free_rate) - \n                (CORR(r.strategy_return, r.market_return) * AVG(r.market_return - r.risk_free_rate)) as estimated_alpha,\n            CORR(r.strategy_return, r.market_return) as market_beta,\n            CORR(r.strategy_return, r.size_factor) as size_beta,\n            CORR(r.strategy_return, r.value_factor) as value_beta,\n            CORR(r.strategy_return, r.momentum_factor) as momentum_beta\n        FROM renaissance_strategies s\n        JOIN renaissance_returns r ON s.strategy_id = r.strategy_id\n        GROUP BY s.strategy_id, s.strategy_name, s.strategy_type\n    ),\n    regime_analysis AS (\n        SELECT \n            s.strategy_id,\n            r.market_regime,\n            AVG(r.strategy_return - r.risk_free_rate) as regime_excess_return,\n            STDDEV(r.strategy_return - r.risk_free_rate) as regime_volatility,\n            COUNT(*) as regime_observations\n        FROM renaissance_strategies s\n        JOIN renaissance_returns r ON s.strategy_id = r.strategy_id\n        GROUP BY s.strategy_id, r.market_regime\n    ),\n    information_ratios AS (\n        SELECT \n            fr.*,\n            -- Information Ratio = Alpha / Tracking Error (simplified)\n            CASE \n                WHEN fr.return_volatility > 0 THEN fr.estimated_alpha / fr.return_volatility\n                ELSE 0\n            END as information_ratio\n        FROM factor_regressions fr\n    )\n    SELECT \n        ir.strategy_name,\n        ir.strategy_type,\n        ROUND(ir.estimated_alpha * 252, 4) as annualized_alpha,\n        ROUND(ir.market_beta, 3) as market_beta,\n        ROUND(ir.size_beta, 3) as size_beta,  \n        ROUND(ir.value_beta, 3) as value_beta,\n        ROUND(ir.momentum_beta, 3) as momentum_beta,\n        ROUND(ir.information_ratio, 3) as information_ratio,\n        ROUND(ir.return_volatility * SQRT(252), 3) as annualized_volatility,\n        ir.observations\n    FROM information_ratios ir\n    WHERE ir.information_ratio > 1.5\n    ORDER BY ir.information_ratio DESC;	Advanced quantitative finance using multi-factor models, alpha decomposition, Information Ratio calculations, and regime analysis for systematic trading strategy evaluation.	2025-08-11 09:51:21.537722	\N
07f3a466-1be4-4445-abe7-d36d40f6be0f	fefcc89f-08b7-4473-857b-67a895af8b8b	postgresql	CREATE TABLE berkshire_insurance_lines (\n        line_id INT PRIMARY KEY,\n        business_line VARCHAR(100),\n        line_manager VARCHAR(100),\n        regulatory_capital_req DECIMAL(15,2),\n        avg_policy_duration_years DECIMAL(4,2)\n    );\n    \n    CREATE TABLE berkshire_float_data (\n        data_id INT PRIMARY KEY,\n        line_id INT,\n        reporting_period VARCHAR(20),\n        premiums_written DECIMAL(15,2),\n        claims_paid DECIMAL(15,2),\n        expenses DECIMAL(15,2),\n        float_balance DECIMAL(15,2),\n        investment_income DECIMAL(15,2),\n        underwriting_result DECIMAL(15,2)\n    );\n    \n    INSERT INTO berkshire_insurance_lines VALUES \n    (1, 'GEICO Auto Insurance', 'Tony Nicely', 15000000000.00, 1.0),\n    (2, 'General Re Reinsurance', 'Tad Montross', 25000000000.00, 3.5),\n    (3, 'Berkshire Hathaway Reinsurance', 'Ajit Jain', 45000000000.00, 8.2),\n    (4, 'National Indemnity', 'Don Wurster', 8000000000.00, 2.1),\n    (5, 'Medical Protective', 'Tim Kenesey', 3000000000.00, 4.8);\n    \n    INSERT INTO berkshire_float_data VALUES \n    (1, 1, '2023 Annual', 48000000000.00, 42000000000.00, 4500000000.00, 28000000000.00, 2100000000.00, 1500000000.00),\n    (2, 2, '2023 Annual', 8500000000.00, 7200000000.00, 980000000.00, 35000000000.00, 3200000000.00, 320000000.00),\n    (3, 3, '2023 Annual', 12000000000.00, 9800000000.00, 1100000000.00, 78000000000.00, 6800000000.00, 1100000000.00),\n    (4, 4, '2023 Annual', 3200000000.00, 2800000000.00, 450000000.00, 12000000000.00, 950000000.00, -50000000.00),\n    (5, 5, '2023 Annual', 1800000000.00, 1400000000.00, 320000000.00, 8500000000.00, 720000000.00, 80000000.00);	WITH insurance_metrics AS (\n        SELECT \n            il.line_id,\n            il.business_line,\n            il.line_manager,\n            il.avg_policy_duration_years,\n            fd.premiums_written,\n            fd.claims_paid,\n            fd.expenses,\n            fd.float_balance,\n            fd.investment_income,\n            fd.underwriting_result,\n            -- Combined Ratio = (Claims + Expenses) / Premiums Written\n            ((fd.claims_paid + fd.expenses) / fd.premiums_written) * 100 as combined_ratio,\n            -- Float Cost = Underwriting Loss / Float Balance (if underwriting loss)\n            CASE \n                WHEN fd.underwriting_result < 0 THEN ABS(fd.underwriting_result) / fd.float_balance\n                ELSE 0\n            END as float_cost_rate,\n            -- Investment Return Rate\n            fd.investment_income / fd.float_balance as investment_return_rate,\n            -- Capital Efficiency = Float Balance / Regulatory Capital\n            fd.float_balance / il.regulatory_capital_req as capital_efficiency\n        FROM berkshire_insurance_lines il\n        JOIN berkshire_float_data fd ON il.line_id = fd.line_id\n    ),\n    float_value_analysis AS (\n        SELECT \n            *,\n            -- Net Float Yield = Investment Return - Float Cost\n            (investment_return_rate - float_cost_rate) as net_float_yield,\n            -- Float Value = Float × Net Yield × Duration (simplified)\n            float_balance * (investment_return_rate - float_cost_rate) * avg_policy_duration_years as float_value,\n            -- Regulatory Capital ROE\n            (investment_income + underwriting_result) / (float_balance / capital_efficiency) as regulatory_roe\n        FROM insurance_metrics\n    )\n    SELECT \n        business_line,\n        line_manager,\n        ROUND(float_balance / 1000000000, 1) as float_balance_billions,\n        ROUND(combined_ratio, 2) as combined_ratio,\n        ROUND(net_float_yield * 100, 2) as float_yield_pct,\n        ROUND(float_value / 1000000000, 1) as float_value_billions,\n        ROUND(investment_return_rate * 100, 2) as investment_return_pct,\n        ROUND(capital_efficiency, 2) as capital_efficiency,\n        ROUND(regulatory_roe * 100, 2) as regulatory_roe_pct\n    FROM float_value_analysis\n    WHERE net_float_yield > 0.08 \n        AND combined_ratio < 100\n    ORDER BY float_value DESC;	Insurance industry financial analysis using float valuation models, combined ratio calculations, and capital efficiency metrics for investment-focused insurance operations optimization.	2025-08-11 09:51:21.538839	\N
932a0274-eb18-4fe3-8e5d-41bca45161cd	4e721623-1caa-49da-ad07-cf77cb0fdfc0	postgresql	CREATE TABLE apple_sales (sale_id INT, quarter VARCHAR(10), iphone_model VARCHAR(50), units_sold INT, revenue DECIMAL(15,2));\n    INSERT INTO apple_sales VALUES (1, 'Q1 2024', 'iPhone 15', 12500, 999750000), (2, 'Q1 2024', 'iPhone 14', 8500, 679150000), (3, 'Q2 2024', 'iPhone 15 Pro', 15000, 1499250000);	SELECT quarter, SUM(units_sold) as total_units FROM apple_sales GROUP BY quarter ORDER BY total_units DESC;	Basic Queries analysis for Apple iPhone Sales by Quarter	2025-08-11 13:06:42.243893	\N
f90bdc1f-0cb7-427f-9605-f825cff78f87	2abba901-541a-4c0e-ae30-624927a7b3f3	postgresql	CREATE TABLE amazon_memberships (member_id INT, region VARCHAR(30), membership_type VARCHAR(20), signup_date DATE, monthly_fee DECIMAL(6,2));\n    INSERT INTO amazon_memberships VALUES (1, 'North America', 'Prime', '2024-01-15', 14.99), (2, 'Europe', 'Prime', '2024-01-20', 8.99), (3, 'Asia Pacific', 'Prime Student', '2024-02-10', 7.49);	SELECT region, COUNT(*) as member_count FROM amazon_memberships GROUP BY region ORDER BY member_count DESC;	Aggregation analysis for Amazon Prime Membership Growth	2025-08-11 13:06:42.245881	\N
f0719033-7e71-416c-92d5-d36c0276644f	2b252b01-350c-481f-a78a-b0259d68e69a	postgresql	CREATE TABLE google_ads (ad_id INT, platform VARCHAR(30), advertiser VARCHAR(100), revenue DECIMAL(12,2), impressions BIGINT, clicks INT);\n    INSERT INTO google_ads VALUES (1, 'Search', 'Nike', 125000.50, 5000000, 45000), (2, 'YouTube', 'Coca Cola', 89000.25, 8000000, 32000), (3, 'Display Network', 'Amazon', 67000.75, 3500000, 28000);	SELECT platform, SUM(revenue) as total_revenue FROM google_ads GROUP BY platform ORDER BY total_revenue DESC;	Basic Queries analysis for Google Ad Revenue by Platform	2025-08-11 13:06:42.247251	\N
aa88bf67-6d71-4ed6-8db5-4e4563adc0f6	24bde9ed-3686-42bc-8256-0623c8140863	postgresql	CREATE TABLE walmart_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO walmart_revenue VALUES (1, 'Core Business', 673442.50, 'Q1 2024'), (2, 'Digital Services', 358548.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM walmart_revenue GROUP BY segment ORDER BY total_revenue DESC;	Basic Queries analysis for Fortune 100 companies	2025-08-11 13:08:07.746897	\N
f9b44e21-f93d-497e-9ef1-f64bb517c5cc	55c0bc97-b6bc-42ae-a1dd-b08f0cec5e82	postgresql	CREATE TABLE airbnb_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO airbnb_revenue VALUES (1, 'Core Business', 567610.50, 'Q1 2024'), (2, 'Digital Services', 630723.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM airbnb_revenue GROUP BY segment ORDER BY total_revenue DESC;	Basic Queries analysis for Fortune 100 companies	2025-08-11 13:08:07.818674	\N
a32b6e9e-d464-42c1-b239-35c179836fd6	7ceae8af-c0b8-49de-afca-daa1af1c484c	postgresql	CREATE TABLE spotify_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO spotify_revenue VALUES (1, 'Core Business', 1348515.50, 'Q1 2024'), (2, 'Digital Services', 525055.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM spotify_revenue GROUP BY segment ORDER BY total_revenue DESC;	Aggregation analysis for Fortune 100 companies	2025-08-11 13:08:07.819691	\N
8b3d757f-5b5d-4ae3-b3ba-e2ca16333f0a	a2dedad9-60f7-4e85-84a2-90ea11c097d6	postgresql	CREATE TABLE adobe_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO adobe_revenue VALUES (1, 'Core Business', 1383985.50, 'Q1 2024'), (2, 'Digital Services', 510734.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM adobe_revenue GROUP BY segment ORDER BY total_revenue DESC;	Basic Queries analysis for Fortune 100 companies	2025-08-11 13:08:07.820742	\N
53609ad0-e125-49b7-a120-64678451f46d	0a009921-29bc-46b5-a63f-bd014ae9782e	postgresql	CREATE TABLE salesforce_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO salesforce_revenue VALUES (1, 'Core Business', 937521.50, 'Q1 2024'), (2, 'Digital Services', 295867.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM salesforce_revenue GROUP BY segment ORDER BY total_revenue DESC;	Aggregation analysis for Fortune 100 companies	2025-08-11 13:08:07.821559	\N
232f44ac-6814-4ba1-a214-a14432680e38	070b96e1-0cb0-4397-9746-275d5937653d	postgresql	CREATE TABLE oracle_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO oracle_revenue VALUES (1, 'Core Business', 715330.50, 'Q1 2024'), (2, 'Digital Services', 469062.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM oracle_revenue GROUP BY segment ORDER BY total_revenue DESC;	Basic Queries analysis for Fortune 100 companies	2025-08-11 13:08:07.822547	\N
996a8a0f-ad8b-4492-bccd-739fa07df3b4	472fcd4b-49f2-4b9d-9344-f78334b99c71	postgresql	CREATE TABLE intel_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO intel_revenue VALUES (1, 'Core Business', 1077495.50, 'Q1 2024'), (2, 'Digital Services', 589740.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM intel_revenue GROUP BY segment ORDER BY total_revenue DESC;	Aggregation analysis for Fortune 100 companies	2025-08-11 13:08:07.823317	\N
7c4b1f56-a5db-4721-8661-3dfa5bf5adb3	bb076210-d02e-4735-8992-4157596ede26	postgresql	CREATE TABLE cisco_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO cisco_revenue VALUES (1, 'Core Business', 600895.50, 'Q1 2024'), (2, 'Digital Services', 255847.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM cisco_revenue GROUP BY segment ORDER BY total_revenue DESC;	Basic Queries analysis for Fortune 100 companies	2025-08-11 13:08:07.824051	\N
64400563-76f7-4c58-8ef6-2610dc8a44f4	63817a18-ce68-4292-8ff7-58fa600d955f	postgresql	CREATE TABLE paypal_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO paypal_revenue VALUES (1, 'Core Business', 820432.50, 'Q1 2024'), (2, 'Digital Services', 467287.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM paypal_revenue GROUP BY segment ORDER BY total_revenue DESC;	Aggregation analysis for Fortune 100 companies	2025-08-11 13:08:07.847817	\N
f87a0a43-984d-434c-9a93-5fb8f084a7e4	b7912212-e1cb-4184-ac6d-c455b85fd13f	postgresql	CREATE TABLE ebay_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO ebay_revenue VALUES (1, 'Core Business', 1317301.50, 'Q1 2024'), (2, 'Digital Services', 603661.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM ebay_revenue GROUP BY segment ORDER BY total_revenue DESC;	Basic Queries analysis for Fortune 100 companies	2025-08-11 13:08:07.850375	\N
420bac10-de14-40ca-a887-3f2e6a9d0f97	b04dd9eb-8254-42e8-979f-bbb6660973a8	postgresql	CREATE TABLE twitter_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO twitter_revenue VALUES (1, 'Core Business', 568643.50, 'Q1 2024'), (2, 'Digital Services', 565876.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM twitter_revenue GROUP BY segment ORDER BY total_revenue DESC;	Aggregation analysis for Fortune 100 companies	2025-08-11 13:08:07.851008	\N
44eeb7c3-f249-4a9c-85fc-96841b338226	19355790-d9ce-46b6-b697-fcce53a43564	postgresql	CREATE TABLE linkedin_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO linkedin_revenue VALUES (1, 'Core Business', 1379900.50, 'Q1 2024'), (2, 'Digital Services', 369438.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM linkedin_revenue GROUP BY segment ORDER BY total_revenue DESC;	Basic Queries analysis for Fortune 100 companies	2025-08-11 13:08:07.851886	\N
4283dafe-509c-4abe-9ca3-35006cca0f61	07d35229-917c-44e8-8f6d-7aaff85c1db6	postgresql	CREATE TABLE snapchat_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO snapchat_revenue VALUES (1, 'Core Business', 1040525.50, 'Q1 2024'), (2, 'Digital Services', 455644.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM snapchat_revenue GROUP BY segment ORDER BY total_revenue DESC;	Aggregation analysis for Fortune 100 companies	2025-08-11 13:08:07.852632	\N
bda5d5e3-f3f4-425b-8cda-2d35fb8a46ed	5df18900-5267-4b6e-bc2b-e5fda74da97c	postgresql	CREATE TABLE pinterest_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO pinterest_revenue VALUES (1, 'Core Business', 1454611.50, 'Q1 2024'), (2, 'Digital Services', 587267.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM pinterest_revenue GROUP BY segment ORDER BY total_revenue DESC;	Basic Queries analysis for Fortune 100 companies	2025-08-11 13:08:07.853231	\N
a04e6869-8f4d-4d19-b9d1-7f1b4f7c0660	4f1e3531-bc31-402a-98c0-527f35acab45	postgresql	CREATE TABLE zoom_revenue (segment_id INT, segment VARCHAR(50), revenue DECIMAL(12,2), quarter VARCHAR(10)); INSERT INTO zoom_revenue VALUES (1, 'Core Business', 547772.50, 'Q1 2024'), (2, 'Digital Services', 316714.25, 'Q1 2024');	SELECT segment, SUM(revenue) as total_revenue FROM zoom_revenue GROUP BY segment ORDER BY total_revenue DESC;	Aggregation analysis for Fortune 100 companies	2025-08-11 13:08:07.853799	\N
ee225772-8120-4aad-820a-af87b9f23f90	c13c9bc3-408d-46b8-8c64-340768bcd065	postgresql	CREATE TABLE blackrock_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO blackrock_portfolio VALUES (1, 'US Equities', 0.4500, 0.0817, 0.1090), (2, 'Bonds', 0.3500, 0.0313, 0.687);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM blackrock_portfolio ORDER BY return_rate DESC;	Window Functions analysis for Fortune 100 companies	2025-08-11 13:08:07.867076	\N
02417807-a794-419e-bd3b-fc766964a08f	4201f984-393a-4772-91eb-90ecd63a3030	postgresql	CREATE TABLE vanguard_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO vanguard_portfolio VALUES (1, 'US Equities', 0.4500, 0.086, 0.1146), (2, 'Bonds', 0.3500, 0.0339, 0.638);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM vanguard_portfolio ORDER BY return_rate DESC;	Joins analysis for Fortune 100 companies	2025-08-11 13:08:07.868826	\N
38a457c7-40e4-4c94-b53c-067c7db32536	9c9399dc-dc2c-4d6e-8ac0-15ffb247433a	postgresql	CREATE TABLE state_street_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO state_street_portfolio VALUES (1, 'US Equities', 0.4500, 0.0831, 0.1408), (2, 'Bonds', 0.3500, 0.0329, 0.700);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM state_street_portfolio ORDER BY return_rate DESC;	Time Analysis analysis for Fortune 100 companies	2025-08-11 13:08:07.869822	\N
2bdacf78-1acb-4ccf-89b5-686ee0d863cb	642f18fd-a78f-49d2-aded-f876ab91e6e2	postgresql	CREATE TABLE fidelity_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO fidelity_portfolio VALUES (1, 'US Equities', 0.4500, 0.0829, 0.1486), (2, 'Bonds', 0.3500, 0.0333, 0.622);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM fidelity_portfolio ORDER BY return_rate DESC;	Subqueries analysis for Fortune 100 companies	2025-08-11 13:08:07.870499	\N
43607949-a863-4c57-9618-74cb5685b703	7cd37118-1e89-4efb-b6a7-8b195e927f66	postgresql	CREATE TABLE charles_schwab_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO charles_schwab_portfolio VALUES (1, 'US Equities', 0.4500, 0.0824, 0.1346), (2, 'Bonds', 0.3500, 0.0331, 0.613);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM charles_schwab_portfolio ORDER BY return_rate DESC;	Window Functions analysis for Fortune 100 companies	2025-08-11 13:08:07.871009	\N
7fe174b9-1206-4577-b7f7-7856d42fcea7	7faab40d-5472-4771-a2ce-80acb990ae52	postgresql	CREATE TABLE morgan_stanley_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO morgan_stanley_portfolio VALUES (1, 'US Equities', 0.4500, 0.0819, 0.1027), (2, 'Bonds', 0.3500, 0.036, 0.671);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM morgan_stanley_portfolio ORDER BY return_rate DESC;	Joins analysis for Fortune 100 companies	2025-08-11 13:08:07.871572	\N
898a0210-5288-4fe2-a30b-641777f162de	e4783bb1-13fb-4dd1-be1f-a07cec187ebf	postgresql	CREATE TABLE bank_of_america_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO bank_of_america_portfolio VALUES (1, 'US Equities', 0.4500, 0.0841, 0.1153), (2, 'Bonds', 0.3500, 0.0318, 0.646);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM bank_of_america_portfolio ORDER BY return_rate DESC;	Time Analysis analysis for Fortune 100 companies	2025-08-11 13:08:07.872071	\N
0553e2f3-46c7-4632-8dcc-dd51c54d2234	c70ce01f-a0c6-4231-b7f1-16576810ba1b	postgresql	CREATE TABLE wells_fargo_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO wells_fargo_portfolio VALUES (1, 'US Equities', 0.4500, 0.0811, 0.1409), (2, 'Bonds', 0.3500, 0.0321, 0.711);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM wells_fargo_portfolio ORDER BY return_rate DESC;	Subqueries analysis for Fortune 100 companies	2025-08-11 13:08:07.87255	\N
688596b9-1fed-481b-a3b6-a9cdc1a48f91	c0a60c80-ad7b-45ff-b137-8d138d785a29	postgresql	CREATE TABLE citigroup_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO citigroup_portfolio VALUES (1, 'US Equities', 0.4500, 0.0842, 0.1315), (2, 'Bonds', 0.3500, 0.0317, 0.667);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM citigroup_portfolio ORDER BY return_rate DESC;	Window Functions analysis for Fortune 100 companies	2025-08-11 13:08:07.873036	\N
17302ce5-39bd-40e8-84f4-00afb61b15b6	a57277ea-6279-49cf-a268-f31a1152e97c	postgresql	CREATE TABLE jpmorgan_chase_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO jpmorgan_chase_portfolio VALUES (1, 'US Equities', 0.4500, 0.0814, 0.1316), (2, 'Bonds', 0.3500, 0.0317, 0.519);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM jpmorgan_chase_portfolio ORDER BY return_rate DESC;	Joins analysis for Fortune 100 companies	2025-08-11 13:08:07.873594	\N
50ff5704-c4c6-43f7-836c-a2df36847241	0f14418a-640f-4711-b5a4-e5b3d81b27d3	postgresql	CREATE TABLE american_express_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO american_express_portfolio VALUES (1, 'US Equities', 0.4500, 0.0811, 0.1136), (2, 'Bonds', 0.3500, 0.0345, 0.716);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM american_express_portfolio ORDER BY return_rate DESC;	Time Analysis analysis for Fortune 100 companies	2025-08-11 13:08:07.874136	\N
1ec2e3dc-a77e-4675-90b1-e8ce7ee627d4	71efa828-8725-4cdd-9311-c22fb8d71038	postgresql	CREATE TABLE visa_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO visa_portfolio VALUES (1, 'US Equities', 0.4500, 0.0839, 0.1472), (2, 'Bonds', 0.3500, 0.0325, 0.683);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM visa_portfolio ORDER BY return_rate DESC;	Subqueries analysis for Fortune 100 companies	2025-08-11 13:08:07.874734	\N
2b38ae58-2c1f-4f37-8b63-a5052a8c6e7e	3ff6929e-490a-477b-a060-b0d8873c9cc7	postgresql	CREATE TABLE mastercard_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO mastercard_portfolio VALUES (1, 'US Equities', 0.4500, 0.083, 0.1263), (2, 'Bonds', 0.3500, 0.0310, 0.644);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM mastercard_portfolio ORDER BY return_rate DESC;	Window Functions analysis for Fortune 100 companies	2025-08-11 13:08:07.876914	\N
f21fd6a5-1d5a-48fd-8e8c-539f3c9f6219	bc199c77-a62e-4b78-839b-7ff588d611b7	postgresql	CREATE TABLE berkshire_hathaway_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO berkshire_hathaway_portfolio VALUES (1, 'US Equities', 0.4500, 0.0829, 0.1142), (2, 'Bonds', 0.3500, 0.0317, 0.513);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM berkshire_hathaway_portfolio ORDER BY return_rate DESC;	Joins analysis for Fortune 100 companies	2025-08-11 13:08:07.877428	\N
aff1b051-dc4e-4518-aff0-154faab7c551	775624c4-7e3c-4409-ab16-3c0060ea17b7	postgresql	CREATE TABLE johnson___johnson_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO johnson___johnson_portfolio VALUES (1, 'US Equities', 0.4500, 0.0827, 0.1049), (2, 'Bonds', 0.3500, 0.0319, 0.632);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM johnson___johnson_portfolio ORDER BY return_rate DESC;	Time Analysis analysis for Fortune 100 companies	2025-08-11 13:08:07.878015	\N
2d745211-bbf2-4350-90b2-9706c59de9e3	c1a0b7de-a008-4edb-9789-ff200664e864	postgresql	CREATE TABLE pfizer_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO pfizer_portfolio VALUES (1, 'US Equities', 0.4500, 0.0815, 0.1067), (2, 'Bonds', 0.3500, 0.0317, 0.684);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM pfizer_portfolio ORDER BY return_rate DESC;	Subqueries analysis for Fortune 100 companies	2025-08-11 13:08:07.878489	\N
7eec873f-7ea0-4cea-b664-67fa9d242af1	ca5c8e64-0773-49e7-8e2b-4dfb2390b56b	postgresql	CREATE TABLE merck_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO merck_portfolio VALUES (1, 'US Equities', 0.4500, 0.0836, 0.1412), (2, 'Bonds', 0.3500, 0.0331, 0.714);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM merck_portfolio ORDER BY return_rate DESC;	Window Functions analysis for Fortune 100 companies	2025-08-11 13:08:07.878984	\N
e531206d-4f06-4f96-8f96-00bd7bce84e9	cfc51c10-9765-4ddf-9b33-9b0609081da0	postgresql	CREATE TABLE abbott_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO abbott_portfolio VALUES (1, 'US Equities', 0.4500, 0.080, 0.1239), (2, 'Bonds', 0.3500, 0.0326, 0.541);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM abbott_portfolio ORDER BY return_rate DESC;	Joins analysis for Fortune 100 companies	2025-08-11 13:08:07.879646	\N
46818b2e-c605-47c8-a6cf-857dadf277ea	d82ba28e-c5e9-4d4b-a868-9eb70cff8b13	postgresql	CREATE TABLE bristol_myers_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO bristol_myers_portfolio VALUES (1, 'US Equities', 0.4500, 0.0822, 0.1132), (2, 'Bonds', 0.3500, 0.0330, 0.638);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM bristol_myers_portfolio ORDER BY return_rate DESC;	Time Analysis analysis for Fortune 100 companies	2025-08-11 13:08:07.880201	\N
34146a38-ee9e-4bdb-9f7b-b221e2e9955d	f7c6524a-7892-49c7-8f3d-c6bf1f610d4f	postgresql	CREATE TABLE eli_lilly_portfolio (asset_id INT, asset_name VARCHAR(50), allocation DECIMAL(8,4), return_rate DECIMAL(8,6), risk_score DECIMAL(6,4)); INSERT INTO eli_lilly_portfolio VALUES (1, 'US Equities', 0.4500, 0.082, 0.1114), (2, 'Bonds', 0.3500, 0.0317, 0.677);	SELECT asset_name, allocation * 100 as allocation_pct, ROUND(return_rate * 100, 2) as return_pct FROM eli_lilly_portfolio ORDER BY return_rate DESC;	Subqueries analysis for Fortune 100 companies	2025-08-11 13:08:07.880816	\N
24e41ec7-eb7c-47f9-af5b-b3642418fbb7	c9150191-e1c2-428e-bfa7-10cd2aca4516	postgresql	CREATE TABLE goldman_sachs_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO goldman_sachs_risk VALUES (1, 'Interest Rate Swap', 62053117.00, 5321701.00, 573880.00, 648556.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM goldman_sachs_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.881433	\N
2e839164-d51b-4372-8bba-6247e701626d	21e44923-0f54-4782-937c-59917622752b	postgresql	CREATE TABLE morgan_stanley_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO morgan_stanley_risk VALUES (1, 'Interest Rate Swap', 141515418.00, 5354603.00, 138133.00, 760750.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM morgan_stanley_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.881947	\N
43801967-b3c2-4f2e-89cf-4b75b3356489	f2ba73f5-2964-42ae-8705-2ea1aff2001b	postgresql	CREATE TABLE credit_suisse_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO credit_suisse_risk VALUES (1, 'Interest Rate Swap', 120974199.00, 3213901.00, 500429.00, 514277.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM credit_suisse_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.882424	\N
180470fe-51e3-4d64-b07c-020f592e7e5b	614e2862-eac5-4ea0-96e3-f45f2c44d46e	postgresql	CREATE TABLE ubs_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO ubs_risk VALUES (1, 'Interest Rate Swap', 73576301.00, 4964887.00, 432756.00, 441467.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM ubs_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.882956	\N
f114bb70-6158-4d8f-9bc9-972fa74dc895	48c005a7-d8da-47b7-b266-8abc09bc7ba9	postgresql	CREATE TABLE deutsche_bank_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO deutsche_bank_risk VALUES (1, 'Interest Rate Swap', 59501371.00, 5130458.00, 467476.00, 769734.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM deutsche_bank_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.883732	\N
e07a60ca-0c62-415b-a371-dcdddfb14bbd	35bf652f-f0c5-42b7-a1df-6c7936a7bf19	postgresql	CREATE TABLE barclays_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO barclays_risk VALUES (1, 'Interest Rate Swap', 122978971.00, 5624882.00, 200127.00, 386340.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM barclays_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.884264	\N
1b20964f-dc28-43cc-90ba-607431c71bf2	e3b08029-a646-4855-97f3-ad8fc7a1b300	postgresql	CREATE TABLE hsbc_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO hsbc_risk VALUES (1, 'Interest Rate Swap', 69090048.00, 1495999.00, 495890.00, 262448.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM hsbc_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.885241	\N
ec2cd8cd-04cc-4cfa-925d-ad86d1f1b60c	d52d9256-38a1-4a27-9d80-9cc3edb2e076	postgresql	CREATE TABLE bnp_paribas_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO bnp_paribas_risk VALUES (1, 'Interest Rate Swap', 70646081.00, 2562159.00, 376080.00, 879111.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM bnp_paribas_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.886043	\N
79e802c9-8b37-47b2-9835-d6c48a17db0d	a62c7efb-6dc5-40ae-8261-6cecdd2a482e	postgresql	CREATE TABLE societe_generale_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO societe_generale_risk VALUES (1, 'Interest Rate Swap', 57659308.00, 5483255.00, 402510.00, 418700.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM societe_generale_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.88713	\N
53526b70-7766-4c59-b6e0-f1bea1c194cd	1f1ecb19-0ca8-4831-918f-0385bda358bf	postgresql	CREATE TABLE ing_group_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO ing_group_risk VALUES (1, 'Interest Rate Swap', 79822904.00, 2920405.00, 236529.00, 692971.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM ing_group_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.887892	\N
a0abf367-56c8-43a6-aa59-11500f4c6068	7f371be0-5b37-466d-b402-e780bc583742	postgresql	CREATE TABLE santander_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO santander_risk VALUES (1, 'Interest Rate Swap', 111660246.00, 2948344.00, 542382.00, 448013.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM santander_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.888715	\N
4d99d054-8b9c-4eba-8b10-2fa05e97f1d0	ad4cdc97-b86c-47f2-aaf5-4bb66bab4b20	postgresql	CREATE TABLE bbva_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO bbva_risk VALUES (1, 'Interest Rate Swap', 62691422.00, 4064805.00, 479989.00, 416040.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM bbva_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.889409	\N
1e652e3a-59fe-4b46-a8ab-c4493ef34eae	8594b69b-4955-4f85-8f3b-cca9da9db830	postgresql	CREATE TABLE unicredit_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO unicredit_risk VALUES (1, 'Interest Rate Swap', 101236082.00, 1420475.00, 377531.00, 227723.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM unicredit_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.890267	\N
d7774aba-4f6e-45f9-88b8-888b279045f5	130642c4-caa3-4fb9-8069-7b29b664b0b6	postgresql	CREATE TABLE intesa_sanpaolo_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO intesa_sanpaolo_risk VALUES (1, 'Interest Rate Swap', 59268814.00, 4180938.00, 240905.00, 414099.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM intesa_sanpaolo_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.891137	\N
d3e83f89-b973-4d6d-8495-b37f4cfdab58	0ce766e5-5495-4f79-b455-308809cb24d1	postgresql	CREATE TABLE nordea_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO nordea_risk VALUES (1, 'Interest Rate Swap', 125376830.00, 5994096.00, 244874.00, 578799.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM nordea_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.892071	\N
a2a588e7-9147-401f-8cae-a4138974bc04	d8bd7205-68e2-4567-8db7-bb1613df16f8	postgresql	CREATE TABLE seb_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO seb_risk VALUES (1, 'Interest Rate Swap', 145932223.00, 3376135.00, 209414.00, 560399.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM seb_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.892757	\N
d158a9ef-ddda-4dd3-9c8e-0a4528b07c35	076d0462-bff2-4b4b-a1fb-5f4fec111ea7	postgresql	CREATE TABLE danske_bank_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO danske_bank_risk VALUES (1, 'Interest Rate Swap', 129196304.00, 1318021.00, 132044.00, 794638.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM danske_bank_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.893315	\N
8dbadd6e-e484-4e61-bbf6-060ef0bfb53b	d277eb23-10d8-4551-b82b-87b041897e84	postgresql	CREATE TABLE abn_amro_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO abn_amro_risk VALUES (1, 'Interest Rate Swap', 103761902.00, 3202124.00, 586142.00, 526322.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM abn_amro_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.893863	\N
58a65681-e892-40c7-bec1-682825ec655e	c5750767-06f3-4cb6-96b1-03de6ddfd977	postgresql	CREATE TABLE kbc_group_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO kbc_group_risk VALUES (1, 'Interest Rate Swap', 77436898.00, 4689521.00, 359655.00, 694011.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM kbc_group_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.89447	\N
e030eb4e-1f0e-4f32-b6db-e7bc6108c103	f6a716db-62ef-40fd-87c5-0bb2cceef980	postgresql	CREATE TABLE commerzbank_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO commerzbank_risk VALUES (1, 'Interest Rate Swap', 105259224.00, 1800186.00, 351696.00, 479564.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM commerzbank_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.895083	\N
b8eb0584-d9af-4d42-a593-e4962f0a3de4	8fcc44d6-3682-414a-91d2-55c184b6b3d8	postgresql	CREATE TABLE rabobank_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO rabobank_risk VALUES (1, 'Interest Rate Swap', 72044513.00, 1014960.00, 541694.00, 410659.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM rabobank_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.895843	\N
6535dcf4-5bd6-4b72-b908-13969fcf3525	ee934c23-de4c-457a-afbe-e6bf602bbf78	postgresql	CREATE TABLE standard_chartered_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO standard_chartered_risk VALUES (1, 'Interest Rate Swap', 86068807.00, 2247422.00, 228594.00, 723903.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM standard_chartered_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.896529	\N
50883f23-2867-4037-b205-2d584b6c21fd	c9929cd5-9715-469c-9293-b56b9a0e6b55	postgresql	CREATE TABLE lloyds_banking_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO lloyds_banking_risk VALUES (1, 'Interest Rate Swap', 141390042.00, 5744529.00, 484662.00, 461454.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM lloyds_banking_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.897125	\N
91549985-d59f-4971-a8f1-e6876a56f320	62a8c083-7287-4f29-b924-057d61ea4397	postgresql	CREATE TABLE royal_bank_scotland_risk (position_id INT, instrument VARCHAR(50), notional DECIMAL(15,2), market_value DECIMAL(15,2), var_95 DECIMAL(12,2), expected_shortfall DECIMAL(12,2)); INSERT INTO royal_bank_scotland_risk VALUES (1, 'Interest Rate Swap', 56879910.00, 2441962.00, 398067.00, 599614.00);	WITH risk_metrics AS (SELECT instrument, AVG(var_95) as avg_var, AVG(expected_shortfall) as avg_es, SUM(ABS(market_value)) as total_exposure FROM royal_bank_scotland_risk GROUP BY instrument) SELECT instrument, ROUND(avg_var/1000, 2) as var_thousands, ROUND(total_exposure/1000000, 2) as exposure_millions FROM risk_metrics ORDER BY total_exposure DESC;	Advanced Topics analysis for Fortune 100 companies	2025-08-11 13:08:07.8977	\N
\.


--
-- Data for Name: problems; Type: TABLE DATA; Schema: public; Owner: ss
--

COPY public.problems (id, title, slug, difficulty, description, is_premium, is_active, category_id, created_at, total_submissions, total_accepted, acceptance_rate, tags, hints) FROM stdin;
6702d933-3efb-48d8-a1c6-5e9189af25c8	Goldman Sachs Trading Performance	goldman-sachs-trading-performance	hard	Goldman Sachs quantitative trading analysis	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 09:45:42.352702	0	0	0.00	\N	\N
f066d916-caef-4477-b5b8-36cd24c0e658	Tesla Manufacturing Efficiency	tesla-manufacturing-efficiency	medium	Tesla production line analysis	f	t	9ba6536c-e307-41f7-8ae0-8e49f3f98d55	2025-08-11 09:45:42.352702	0	0	0.00	\N	\N
c2db86f4-b45e-4958-bdba-934e9bd8949b	High Value Customers	high-value-customers	easy	**Scenario:** Airbnb wants to identify high-value customers for VIP treatment.\n\n**Business Context:** Find customers who have made bookings worth more than $1000 in total.\n\n**Problem:** Return customers with total booking value > $1000.\n\n**Expected Output:** Return customer_id and total_spent, ordered by total_spent descending.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 09:51:03.998276	0	0	0.00	\N	\N
20fc0621-5497-4fc6-b871-05b2ac57a124	Product Inventory Status	product-inventory-status	easy	**Scenario:** Walmart needs to track inventory levels for restocking.\n\n**Business Context:** Identify products that are low in stock (quantity < 50) or out of stock.\n\n**Problem:** Find products with low inventory and categorize their status.\n\n**Expected Output:** Return product_name, quantity, and status ('Out of Stock' if 0, 'Low Stock' if < 50, 'In Stock' otherwise).	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 09:51:04.03128	0	0	0.00	\N	\N
38108e8c-7b5c-4134-b346-ea1d7bc31ee2	User Engagement Metrics	user-engagement-metrics	easy	**Scenario:** LinkedIn wants to measure user engagement across different content types.\n\n**Business Context:** Calculate average engagement rate (likes + comments) per post type.\n\n**Problem:** Find average engagement for each post type.\n\n**Expected Output:** Return post_type and avg_engagement, ordered by avg_engagement descending.	f	t	426fcc68-b403-458f-9afd-5137f772de78	2025-08-11 09:51:04.035619	0	0	0.00	\N	\N
d1092dcc-af01-4f71-9ec6-07be5e714e80	Top Spending Customers by Month	top-spending-customers-by-month	medium	**Scenario:** Stripe wants to identify top spending customers each month for account management.\n\n**Business Context:** Find the highest spending customer for each month in 2024.\n\n**Problem:** Return the customer with the highest total spending for each month.\n\n**Expected Output:** Return month, customer_id, and total_spent for top customer each month.	f	t	9ba6536c-e307-41f7-8ae0-8e49f3f98d55	2025-08-11 09:51:04.041038	0	0	0.00	\N	\N
5a52f8a8-e4d2-4519-958f-9352010eff9d	Session Duration Analysis	session-duration-analysis	medium	**Scenario:** YouTube wants to analyze user session patterns for recommendation algorithms.\n\n**Business Context:** Calculate session statistics including total time, average session duration, and user activity patterns.\n\n**Problem:** Find users with average session duration > 30 minutes and total watch time > 2 hours.\n\n**Expected Output:** Return user_id, session_count, total_minutes, and avg_session_duration for qualifying users.	f	t	47c2009b-81d2-458f-96b0-1a68aee370d6	2025-08-11 09:51:04.044584	0	0	0.00	\N	\N
1b437802-cd3e-4030-901c-74517fd027af	Fraudulent Transaction Detection	fraudulent-transaction-detection	hard	**Scenario:** PayPal's fraud detection team needs to identify suspicious transactions.\n\n**Business Context:** Find transactions that are significantly higher than a user's average transaction amount (more than 3x their average).\n\n**Problem:** Identify potentially fraudulent transactions based on spending patterns.\n\n**Expected Output:** Return user_id, transaction_id, amount, user_avg_amount, and ratio for suspicious transactions.	f	t	089e178f-9baa-41c3-8ed5-626d2f2227d5	2025-08-11 09:51:09.988697	0	0	0.00	\N	\N
31ec673c-d80a-4fa5-b27b-2cfb56d5853c	Movie Recommendation Engine	movie-recommendation-engine	hard	**Scenario:** Disney+ wants to build a content recommendation system.\n\n**Business Context:** Find movies similar to a user's watched content based on genre preferences and ratings.\n\n**Problem:** For users who watched 'Action' movies, recommend other high-rated movies in the same genre they haven't seen.\n\n**Expected Output:** Return movie recommendations for Action movie watchers, excluding movies they've already seen.	f	t	c6117091-f4fa-4c45-925b-8671834d6408	2025-08-11 09:51:10.018351	0	0	0.00	\N	\N
0d48baad-aa87-4f93-9612-328305363c65	Supply Chain Optimization	supply-chain-optimization	medium	**Scenario:** FedEx needs to optimize warehouse inventory levels across regions.\n\n**Business Context:** Calculate reorder points for each product at each warehouse based on demand patterns.\n\n**Problem:** Find products that need restocking (current_stock < reorder_point) at each warehouse.\n\n**Expected Output:** Return warehouse_id, product_name, current_stock, reorder_point, and shortage_amount.	f	t	949357d0-c57c-45fc-93a7-4c350616a3a8	2025-08-11 09:51:10.024129	0	0	0.00	\N	\N
4aeaaea6-093f-41c4-9037-d36e44e11bd1	A/B Test Results Analysis	ab-test-results-analysis	medium	**Scenario:** Facebook's growth team needs to analyze A/B test performance for a new feature.\n\n**Business Context:** Compare conversion rates between control and test groups to determine if the new feature improves user engagement.\n\n**Problem:** Calculate conversion rates for each test group and determine statistical significance.\n\n**Expected Output:** Return group_name, total_users, conversions, conversion_rate for each test group.	f	t	8a31828d-d2e8-44b8-bb9e-636d039bdbcf	2025-08-11 09:51:10.028412	0	0	0.00	\N	\N
5b8ca9e4-c845-416a-bb0b-6429619dcbcc	Energy Consumption Analysis	energy-consumption-analysis	medium	**Scenario:** Tesla's energy division analyzes solar panel performance across installations.\n\n**Business Context:** Calculate energy efficiency and identify underperforming installations for maintenance.\n\n**Problem:** Find installations with below-average energy production efficiency.\n\n**Expected Output:** Return installation_id, avg_daily_production, efficiency_rating, and performance_category.	f	t	672071b9-615d-473b-8985-5fbc5612729e	2025-08-11 09:51:10.032285	0	0	0.00	\N	\N
5c8c0ec6-3d89-4c9d-b360-e449a870df7d	Nike Product Sales by Region	nike-product-sales-by-region	easy	**Business Context:** Nike's regional sales teams track product performance across different markets to optimize inventory distribution and marketing spend allocation.\n\n**Scenario:** You're a sales analyst at Nike analyzing regional performance. The marketing team wants to identify regions with strong sales volumes for upcoming campaign investments.\n\n**Problem:** Find all regions where Nike products have generated more than $500,000 in total sales.\n\n**Expected Output:** Regions with total sales amounts (>$500k only), ordered by sales descending.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 09:51:12.105732	0	0	0.00	\N	\N
387f6300-3716-40e2-85dc-a792cb32a6b1	Merck Drug Development Pipeline	merck-drug-development-pipeline	easy	**Business Context:** Merck's pharmaceutical research division tracks drug development projects across different therapeutic areas to optimize R&D resource allocation and timeline planning.\n\n**Scenario:** You're a research pipeline analyst at Merck analyzing project distribution. The R&D leadership wants to understand which therapeutic areas have the most active research programs.\n\n**Problem:** List all therapeutic areas that have more than 5 active drug development projects.\n\n**Expected Output:** Therapeutic areas with active project counts (>5 projects only), ordered by count descending.	f	t	426fcc68-b403-458f-9afd-5137f772de78	2025-08-11 09:51:12.114298	0	0	0.00	\N	\N
ebd41598-6542-40d8-9b54-c7b010de4ca5	Citibank Credit Card Fraud Detection	citibank-credit-card-fraud-detection	medium	**Business Context:** Citibank's fraud prevention team develops machine learning models to detect suspicious credit card transactions in real-time while minimizing false positives that disrupt customer experience.\n\n**Scenario:** You're a fraud analytics specialist at Citibank analyzing transaction patterns. The security team needs to identify cards with unusual spending velocity that may indicate fraudulent activity.\n\n**Problem:** For each credit card, calculate the rolling 7-day average transaction amount and identify cards where any single transaction exceeds 3x the rolling average on that day.\n\n**Expected Output:** Suspicious transactions with card details, transaction amount, rolling average, and deviation multiple, ordered by deviation descending.	f	t	9ba6536c-e307-41f7-8ae0-8e49f3f98d55	2025-08-11 09:51:12.115427	0	0	0.00	\N	\N
71d86caa-b97f-448e-8282-93dc930abe4b	Lockheed Martin Defense Contracts	lockheed-martin-defense-contracts	medium	**Business Context:** Lockheed Martin's defense contracting division manages complex multi-year projects with various government agencies and must track contract performance across different program phases.\n\n**Scenario:** You're a program analyst at Lockheed Martin analyzing contract portfolio performance. The business development team needs to identify which contract types and agencies provide the most stable revenue streams.\n\n**Problem:** Find government agencies that have awarded contracts totaling more than $1 billion with Lockheed Martin, and calculate the average contract duration for each agency.\n\n**Expected Output:** Agency details with total contract value, contract count, and average duration, only agencies >$1B total, ordered by total value descending.	f	t	8798fdcf-0411-45cb-83dd-b4912e133354	2025-08-11 09:51:12.116398	0	0	0.00	\N	\N
9992b85b-7f74-4ada-ab73-2daa3ef60e32	Wells Fargo Mortgage Risk Assessment	wells-fargo-mortgage-risk-assessment	hard	**Business Context:** Wells Fargo's mortgage lending division develops sophisticated risk models to assess loan default probability while ensuring fair lending practices and regulatory compliance across different demographic segments.\n\n**Scenario:** You're a senior risk modeling analyst at Wells Fargo building next-generation mortgage risk assessment tools. The credit committee needs to understand how different borrower characteristics interact to predict default risk.\n\n**Problem:** Create a comprehensive risk score using multiple factors: credit score weight (30%), debt-to-income weight (25%), loan-to-value weight (25%), employment history weight (20%). Calculate default probability by risk score deciles and identify deciles with default rates exceeding 8%.\n\n**Expected Output:** Risk score deciles with borrower counts, average risk scores, default rates, and expected loss calculations, showing only high-risk deciles >8% default rate.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 09:51:12.117482	0	0	0.00	\N	\N
eb15df93-f7ca-4177-97d4-2ff06bc7c917	McKinsey Client Engagement Analysis	mckinsey-client-engagement-analysis	hard	**Business Context:** McKinsey & Company analyzes client engagement patterns and consultant utilization across different practice areas to optimize staffing models and identify high-value client relationships for strategic account management.\n\n**Scenario:** You're a senior analytics manager at McKinsey developing insights for the firm's managing partners. The leadership team needs to understand which engagement types and client combinations drive the highest profitability and consultant development.\n\n**Problem:** Calculate engagement profitability using: Revenue - (Consultant Hours × Hourly Rate × 1.4 overhead multiplier). Identify client-practice combinations with >$2M annual revenue and >25% profit margins. Rank by profit margin and include consultant utilization metrics.\n\n**Expected Output:** High-value client-practice combinations with revenue, costs, profit margins, and utilization rates, showing only profitable engagements meeting criteria, ordered by profit margin descending.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 09:51:12.118744	0	0	0.00	\N	\N
77b5ad49-9a65-4c07-8d89-3a98b9c6040c	Verizon Network Coverage Analysis	verizon-network-coverage-analysis	easy	**Business Context:** Verizon's network operations team monitors cellular tower performance across different regions to optimize coverage and identify areas needing infrastructure investment.\n\n**Scenario:** You're a network analyst at Verizon analyzing tower performance metrics. The infrastructure team needs to identify regions with strong coverage for capacity planning.\n\n**Problem:** Find all regions where Verizon has more than 100 total cellular towers deployed.\n\n**Expected Output:** Regions with tower counts (>100 towers only), ordered by tower count descending.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 09:51:15.977266	0	0	0.00	\N	\N
46126968-4455-4602-821b-06375609254f	AT&T Customer Service Call Volume	at-t-customer-service-call-volume	easy	**Business Context:** AT&T's customer service division tracks call volumes across different service categories to optimize staffing levels and improve customer satisfaction scores.\n\n**Scenario:** You're a customer service analyst at AT&T analyzing call center performance. The operations team wants to identify service categories with high call volumes for resource allocation.\n\n**Problem:** List all service categories that have received more than 1,000 total customer calls.\n\n**Expected Output:** Service categories with total call counts (>1,000 calls only), ordered by call volume descending.	f	t	426fcc68-b403-458f-9afd-5137f772de78	2025-08-11 09:51:15.989765	0	0	0.00	\N	\N
b04dd9eb-8254-42e8-979f-bbb6660973a8	Twitter Revenue Analysis	twitter-revenue-analysis	easy	Twitter's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	426fcc68-b403-458f-9afd-5137f772de78	2025-08-11 13:08:07.850685	0	0	0.00	\N	\N
58a7cb0d-634f-408b-a5b0-36c0599c2b50	Ford Vehicle Sales Performance	ford-vehicle-sales-performance	easy	**Business Context:** Ford's sales division tracks vehicle performance across different models and regions to optimize production schedules and marketing investments.\n\n**Scenario:** You're a sales analyst at Ford analyzing vehicle sales trends. The manufacturing team needs to identify which vehicle categories are performing well for production planning.\n\n**Problem:** Find all vehicle categories where Ford has sold more than 10,000 total units.\n\n**Expected Output:** Vehicle categories with total units sold (>10,000 units only), ordered by sales descending.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 09:51:15.991645	0	0	0.00	\N	\N
02b4dc7d-b5e4-44c5-8340-93a0d4414680	JP Morgan Trading Desk Performance	jp-morgan-trading-desk-performance	medium	**Business Context:** JP Morgan's trading operations analyze performance across different trading desks and strategies to optimize capital allocation and manage risk exposure effectively.\n\n**Scenario:** You're a trading performance analyst at JP Morgan evaluating desk profitability. The head of trading needs to understand how each trading strategy ranks within its asset class by daily P&L performance.\n\n**Problem:** For each asset class, rank trading strategies by average daily P&L and show the top 2 performing strategies per asset class along with their performance metrics.\n\n**Expected Output:** Top 2 strategies per asset class with rankings, average P&L, and total trading days, ordered by asset class and rank.	f	t	9ba6536c-e307-41f7-8ae0-8e49f3f98d55	2025-08-11 09:51:15.993303	0	0	0.00	\N	\N
7770d8bd-25ce-4caf-908f-6371e1a43e3c	CVS Health Pharmacy Inventory Management	cvs-health-pharmacy-inventory-management	medium	**Business Context:** CVS Health's pharmacy operations manage medication inventory across thousands of locations, requiring sophisticated analysis to prevent stockouts while minimizing holding costs.\n\n**Scenario:** You're a supply chain analyst at CVS Health analyzing pharmacy inventory levels. The operations team needs to identify pharmacies with critical medication shortages across multiple therapeutic categories.\n\n**Problem:** Find pharmacies that have low inventory (less than 30 days supply) for medications in more than 3 different therapeutic categories.\n\n**Expected Output:** Pharmacy details with category count and average days supply, only showing pharmacies with >3 categories having low inventory.	f	t	8798fdcf-0411-45cb-83dd-b4912e133354	2025-08-11 09:51:15.994849	0	0	0.00	\N	\N
41facc79-e158-4b9f-a70d-05908d560d3e	Citadel Hedge Fund Risk Parity Analysis	citadel-hedge-fund-risk-parity-analysis	hard	**Business Context:** Citadel's quantitative research team develops sophisticated risk parity strategies that balance portfolio risk contributions across different asset classes and geographical regions.\n\n**Scenario:** You're a senior quantitative analyst at Citadel building next-generation risk parity models. The portfolio management team needs to understand how different positions contribute to overall portfolio risk and optimize allocations accordingly.\n\n**Problem:** Calculate the risk contribution of each position using: Risk Contribution = Position Weight × Beta × Portfolio Volatility. Identify positions contributing more than 5% to total portfolio risk and calculate optimal rebalancing weights to achieve equal risk contribution.\n\n**Expected Output:** High-risk contributing positions with current and optimal weights, risk contributions, and rebalancing recommendations, ordered by current risk contribution descending.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 09:51:15.99655	0	0	0.00	\N	\N
3e5c2aaf-f454-41cf-8898-ac134e7309d4	AIG Insurance Claims Fraud Detection	aig-insurance-claims-fraud-detection	hard	**Business Context:** AIG's insurance fraud detection division develops machine learning models and statistical analysis to identify potentially fraudulent claims while minimizing false positives that could harm legitimate customers.\n\n**Scenario:** You're a senior fraud analytics specialist at AIG developing predictive models for claim investigation prioritization. The claims review team needs to identify suspicious patterns that warrant detailed investigation.\n\n**Problem:** Create a fraud risk score combining multiple indicators: claim amount vs. policy average (weight 30%), claim frequency (weight 25%), time patterns (weight 25%), and adjuster consistency (weight 20%). Identify claims scoring above the 90th percentile for investigation priority.\n\n**Expected Output:** High-risk claims with individual risk factors, composite fraud scores, and investigation priorities, ordered by fraud score descending.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 09:51:15.998584	0	0	0.00	\N	\N
2a77e25e-17bb-4f89-af67-8634e9ca2153	IBM Watson AI Service Usage	ibm-watson-ai-service-usage	easy	**Business Context:** IBM's Watson AI division tracks service usage across different AI capabilities to understand customer adoption patterns and optimize product development priorities.\n\n**Scenario:** You're an AI product analyst at IBM Watson analyzing service utilization. The product team wants to identify which AI services have strong adoption for capacity planning.\n\n**Problem:** Find all AI service categories that have processed more than 1,000 total API requests.\n\n**Expected Output:** AI service categories with request counts (>1,000 requests only), ordered by requests descending.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 09:51:21.524869	0	0	0.00	\N	\N
e250723c-d9ff-4f9b-81ab-ec5e5f5fddd0	Target Store Revenue by Category	target-store-revenue-by-category	easy	**Business Context:** Target's retail operations team analyzes product category performance across stores to optimize inventory allocation and promotional strategies.\n\n**Scenario:** You're a retail analyst at Target studying category performance. The merchandising team needs to identify which product categories are driving the most revenue for strategic planning.\n\n**Problem:** List all product categories with total revenue exceeding $1 million.\n\n**Expected Output:** Product categories with total revenue (>$1M only), ordered by revenue descending.	f	t	426fcc68-b403-458f-9afd-5137f772de78	2025-08-11 09:51:21.530784	0	0	0.00	\N	\N
20b87983-9952-4f6b-ad79-96cce189c2e2	Capital One Credit Risk Modeling	capital-one-credit-risk-modeling	medium	**Business Context:** Capital One's credit risk team develops sophisticated models to assess borrower creditworthiness and optimize lending decisions while maintaining competitive approval rates.\n\n**Scenario:** You're a credit risk analyst at Capital One analyzing borrower risk profiles. The lending team needs to understand how borrowers rank within their income segments based on credit utilization patterns.\n\n**Problem:** For each income bracket, rank borrowers by their average credit utilization ratio and identify the top 3 highest utilization borrowers per bracket along with their risk metrics.\n\n**Expected Output:** Top 3 borrowers per income bracket with utilization rankings and risk scores, ordered by income bracket and rank.	f	t	9ba6536c-e307-41f7-8ae0-8e49f3f98d55	2025-08-11 09:51:21.532153	0	0	0.00	\N	\N
7102509c-bbd8-4706-99c5-23b599ec9de3	UnitedHealth Claims Processing Efficiency	unitedhealth-claims-processing-efficiency	medium	**Business Context:** UnitedHealth's claims processing division analyzes workflow efficiency across different claim types and processing centers to optimize operations and reduce patient wait times.\n\n**Scenario:** You're an operations analyst at UnitedHealth studying claims processing performance. The operations team needs to identify processing centers with consistently high efficiency across multiple claim types.\n\n**Problem:** Find processing centers that handle more than 5 different claim types AND maintain an average processing time under 3 days across all claim types.\n\n**Expected Output:** Efficient processing centers with claim type diversity, average processing times, and total volume, meeting both criteria.	f	t	8798fdcf-0411-45cb-83dd-b4912e133354	2025-08-11 09:51:21.53439	0	0	0.00	\N	\N
555fe7bc-059f-4b65-aa4f-0e6a318265d8	Renaissance Technologies Quantitative Alpha	renaissance-technologies-quantitative-alpha	hard	**Business Context:** Renaissance Technologies' quantitative research team develops sophisticated mathematical models to identify market inefficiencies and generate alpha through statistical arbitrage strategies.\n\n**Scenario:** You're a senior quantitative researcher at Renaissance Technologies building next-generation alpha generation models. The investment committee needs to understand which factor combinations produce the most consistent risk-adjusted returns across different market regimes.\n\n**Problem:** Calculate factor loadings and alpha generation using a multi-factor model: Return = Alpha + β₁×Market + β₂×Size + β₃×Value + β₄×Momentum + ε. Identify strategies with Information Ratios > 1.5 and consistent alpha across bull/bear markets.\n\n**Expected Output:** High-alpha strategies with factor loadings, Information Ratios, and regime-specific performance, showing only strategies with IR > 1.5, ordered by Information Ratio descending.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 09:51:21.536848	0	0	0.00	\N	\N
fefcc89f-08b7-4473-857b-67a895af8b8b	Berkshire Hathaway Insurance Float Optimization	berkshire-hathaway-insurance-float-optimization	hard	**Business Context:** Berkshire Hathaway's insurance operations generate massive float that Warren Buffett invests to create value. The insurance division must optimize float generation while maintaining underwriting discipline and regulatory capital requirements.\n\n**Scenario:** You're a senior insurance analyst at Berkshire Hathaway developing float optimization models. The executive team needs to understand which business lines generate the most valuable float considering both size and cost characteristics.\n\n**Problem:** Calculate float value using: Float Value = Float Amount × (Investment Return - Float Cost) × Duration. Identify business lines with float yields > 8% and combined ratios < 100%. Include regulatory capital efficiency metrics.\n\n**Expected Output:** High-value insurance lines with float metrics, investment returns, combined ratios, and capital efficiency, showing only profitable float generators, ordered by float value descending.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 09:51:21.53835	0	0	0.00	\N	\N
4e721623-1caa-49da-ad07-cf77cb0fdfc0	Apple iPhone Sales by Quarter	apple-iphone-sales-by-quarter	easy	Apple's retail operations track iPhone sales performance across different quarters to optimize inventory and production planning.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 13:06:42.109191	0	0	0.00	\N	\N
2abba901-541a-4c0e-ae30-624927a7b3f3	Amazon Prime Membership Growth	amazon-prime-membership-growth	easy	Amazon analyzes Prime membership growth across different regions to understand market penetration and expansion opportunities.	f	t	426fcc68-b403-458f-9afd-5137f772de78	2025-08-11 13:06:42.245233	0	0	0.00	\N	\N
2b252b01-350c-481f-a78a-b0259d68e69a	Google Ad Revenue by Platform	google-ad-revenue-by-platform	easy	Google's advertising division tracks revenue performance across different platforms to optimize ad placement strategies.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 13:06:42.24658	0	0	0.00	\N	\N
b01ac3b6-2598-4cc2-9375-cec6a2aa0560	Microsoft Azure Cloud Analytics	microsoft-azure-cloud-analytics	medium	Microsoft Azure analyzes customer usage patterns and growth trends to optimize cloud service offerings and pricing strategies.	f	t	9ba6536c-e307-41f7-8ae0-8e49f3f98d55	2025-08-11 13:06:42.247754	0	0	0.00	\N	\N
addf9669-6c47-4410-8b73-354e852ff005	JPMorgan Derivatives Risk Analytics	jpmorgan-derivatives-risk-analytics	hard	JPMorgan's quantitative risk team develops sophisticated models to measure and manage derivatives portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:06:42.249403	0	0	0.00	\N	\N
24bde9ed-3686-42bc-8256-0623c8140863	Walmart Revenue Analysis	walmart-revenue-analysis	easy	Walmart's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 13:08:07.711794	0	0	0.00	\N	\N
c28cd914-6ca0-4082-9358-090d21a4d66c	Costco Revenue Analysis	costco-revenue-analysis	easy	Costco's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	426fcc68-b403-458f-9afd-5137f772de78	2025-08-11 13:08:07.748681	0	0	0.00	\N	\N
9da41be0-656e-4919-8987-25e333bef4c6	Home Depot Revenue Analysis	home-depot-revenue-analysis	easy	Home Depot's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 13:08:07.772524	0	0	0.00	\N	\N
f8b1fc39-54aa-469b-ad12-af368903bb47	Disney Revenue Analysis	disney-revenue-analysis	easy	Disney's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	426fcc68-b403-458f-9afd-5137f772de78	2025-08-11 13:08:07.774105	0	0	0.00	\N	\N
f5c2c06b-023a-4e07-9240-18f97fefcde2	Netflix Revenue Analysis	netflix-revenue-analysis	easy	Netflix's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 13:08:07.775539	0	0	0.00	\N	\N
15382d01-697d-44bc-bfaa-931fec0cbdae	Uber Revenue Analysis	uber-revenue-analysis	easy	Uber's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	426fcc68-b403-458f-9afd-5137f772de78	2025-08-11 13:08:07.816646	0	0	0.00	\N	\N
55c0bc97-b6bc-42ae-a1dd-b08f0cec5e82	Airbnb Revenue Analysis	airbnb-revenue-analysis	easy	Airbnb's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 13:08:07.818206	0	0	0.00	\N	\N
7ceae8af-c0b8-49de-afca-daa1af1c484c	Spotify Revenue Analysis	spotify-revenue-analysis	easy	Spotify's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	426fcc68-b403-458f-9afd-5137f772de78	2025-08-11 13:08:07.819219	0	0	0.00	\N	\N
a2dedad9-60f7-4e85-84a2-90ea11c097d6	Adobe Revenue Analysis	adobe-revenue-analysis	easy	Adobe's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 13:08:07.820293	0	0	0.00	\N	\N
0a009921-29bc-46b5-a63f-bd014ae9782e	Salesforce Revenue Analysis	salesforce-revenue-analysis	easy	Salesforce's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	426fcc68-b403-458f-9afd-5137f772de78	2025-08-11 13:08:07.82114	0	0	0.00	\N	\N
070b96e1-0cb0-4397-9746-275d5937653d	Oracle Revenue Analysis	oracle-revenue-analysis	easy	Oracle's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 13:08:07.822112	0	0	0.00	\N	\N
472fcd4b-49f2-4b9d-9344-f78334b99c71	Intel Revenue Analysis	intel-revenue-analysis	easy	Intel's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	426fcc68-b403-458f-9afd-5137f772de78	2025-08-11 13:08:07.822908	0	0	0.00	\N	\N
bb076210-d02e-4735-8992-4157596ede26	Cisco Revenue Analysis	cisco-revenue-analysis	easy	Cisco's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 13:08:07.823662	0	0	0.00	\N	\N
63817a18-ce68-4292-8ff7-58fa600d955f	PayPal Revenue Analysis	paypal-revenue-analysis	easy	PayPal's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	426fcc68-b403-458f-9afd-5137f772de78	2025-08-11 13:08:07.824429	0	0	0.00	\N	\N
b7912212-e1cb-4184-ac6d-c455b85fd13f	eBay Revenue Analysis	ebay-revenue-analysis	easy	eBay's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 13:08:07.849851	0	0	0.00	\N	\N
19355790-d9ce-46b6-b697-fcce53a43564	LinkedIn Revenue Analysis	linkedin-revenue-analysis	easy	LinkedIn's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 13:08:07.851488	0	0	0.00	\N	\N
07d35229-917c-44e8-8f6d-7aaff85c1db6	Snapchat Revenue Analysis	snapchat-revenue-analysis	easy	Snapchat's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	426fcc68-b403-458f-9afd-5137f772de78	2025-08-11 13:08:07.852303	0	0	0.00	\N	\N
5df18900-5267-4b6e-bc2b-e5fda74da97c	Pinterest Revenue Analysis	pinterest-revenue-analysis	easy	Pinterest's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	c5ec99f8-01ff-4d36-b36e-27688566397d	2025-08-11 13:08:07.852895	0	0	0.00	\N	\N
4f1e3531-bc31-402a-98c0-527f35acab45	Zoom Revenue Analysis	zoom-revenue-analysis	easy	Zoom's financial team analyzes revenue performance across different business segments to optimize operational strategies.	f	t	426fcc68-b403-458f-9afd-5137f772de78	2025-08-11 13:08:07.853493	0	0	0.00	\N	\N
c13c9bc3-408d-46b8-8c64-340768bcd065	BlackRock Portfolio Analytics	blackrock-portfolio-analytics	medium	BlackRock's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	9ba6536c-e307-41f7-8ae0-8e49f3f98d55	2025-08-11 13:08:07.854101	0	0	0.00	\N	\N
4201f984-393a-4772-91eb-90ecd63a3030	Vanguard Portfolio Analytics	vanguard-portfolio-analytics	medium	Vanguard's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	8798fdcf-0411-45cb-83dd-b4912e133354	2025-08-11 13:08:07.867818	0	0	0.00	\N	\N
9c9399dc-dc2c-4d6e-8ac0-15ffb247433a	State Street Portfolio Analytics	state-street-portfolio-analytics	medium	State Street's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	47c2009b-81d2-458f-96b0-1a68aee370d6	2025-08-11 13:08:07.869331	0	0	0.00	\N	\N
642f18fd-a78f-49d2-aded-f876ab91e6e2	Fidelity Portfolio Analytics	fidelity-portfolio-analytics	medium	Fidelity's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	e1b879e5-e95b-41ee-b22a-a2ea91897277	2025-08-11 13:08:07.870187	0	0	0.00	\N	\N
7cd37118-1e89-4efb-b6a7-8b195e927f66	Charles Schwab Portfolio Analytics	charles-schwab-portfolio-analytics	medium	Charles Schwab's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	9ba6536c-e307-41f7-8ae0-8e49f3f98d55	2025-08-11 13:08:07.870753	0	0	0.00	\N	\N
7faab40d-5472-4771-a2ce-80acb990ae52	Morgan Stanley Portfolio Analytics	morgan-stanley-portfolio-analytics	medium	Morgan Stanley's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	8798fdcf-0411-45cb-83dd-b4912e133354	2025-08-11 13:08:07.871236	0	0	0.00	\N	\N
e4783bb1-13fb-4dd1-be1f-a07cec187ebf	Bank of America Portfolio Analytics	bank-of-america-portfolio-analytics	medium	Bank of America's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	47c2009b-81d2-458f-96b0-1a68aee370d6	2025-08-11 13:08:07.871812	0	0	0.00	\N	\N
c70ce01f-a0c6-4231-b7f1-16576810ba1b	Wells Fargo Portfolio Analytics	wells-fargo-portfolio-analytics	medium	Wells Fargo's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	e1b879e5-e95b-41ee-b22a-a2ea91897277	2025-08-11 13:08:07.872302	0	0	0.00	\N	\N
c0a60c80-ad7b-45ff-b137-8d138d785a29	Citigroup Portfolio Analytics	citigroup-portfolio-analytics	medium	Citigroup's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	9ba6536c-e307-41f7-8ae0-8e49f3f98d55	2025-08-11 13:08:07.872773	0	0	0.00	\N	\N
a57277ea-6279-49cf-a268-f31a1152e97c	JPMorgan Chase Portfolio Analytics	jpmorgan-chase-portfolio-analytics	medium	JPMorgan Chase's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	8798fdcf-0411-45cb-83dd-b4912e133354	2025-08-11 13:08:07.873331	0	0	0.00	\N	\N
0f14418a-640f-4711-b5a4-e5b3d81b27d3	American Express Portfolio Analytics	american-express-portfolio-analytics	medium	American Express's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	47c2009b-81d2-458f-96b0-1a68aee370d6	2025-08-11 13:08:07.873879	0	0	0.00	\N	\N
71efa828-8725-4cdd-9311-c22fb8d71038	Visa Portfolio Analytics	visa-portfolio-analytics	medium	Visa's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	e1b879e5-e95b-41ee-b22a-a2ea91897277	2025-08-11 13:08:07.874431	0	0	0.00	\N	\N
3ff6929e-490a-477b-a060-b0d8873c9cc7	Mastercard Portfolio Analytics	mastercard-portfolio-analytics	medium	Mastercard's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	9ba6536c-e307-41f7-8ae0-8e49f3f98d55	2025-08-11 13:08:07.87503	0	0	0.00	\N	\N
bc199c77-a62e-4b78-839b-7ff588d611b7	Berkshire Hathaway Portfolio Analytics	berkshire-hathaway-portfolio-analytics	medium	Berkshire Hathaway's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	8798fdcf-0411-45cb-83dd-b4912e133354	2025-08-11 13:08:07.877157	0	0	0.00	\N	\N
775624c4-7e3c-4409-ab16-3c0060ea17b7	Johnson & Johnson Portfolio Analytics	johnson-johnson-portfolio-analytics	medium	Johnson & Johnson's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	47c2009b-81d2-458f-96b0-1a68aee370d6	2025-08-11 13:08:07.87776	0	0	0.00	\N	\N
c1a0b7de-a008-4edb-9789-ff200664e864	Pfizer Portfolio Analytics	pfizer-portfolio-analytics	medium	Pfizer's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	e1b879e5-e95b-41ee-b22a-a2ea91897277	2025-08-11 13:08:07.87824	0	0	0.00	\N	\N
ca5c8e64-0773-49e7-8e2b-4dfb2390b56b	Merck Portfolio Analytics	merck-portfolio-analytics	medium	Merck's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	9ba6536c-e307-41f7-8ae0-8e49f3f98d55	2025-08-11 13:08:07.878725	0	0	0.00	\N	\N
cfc51c10-9765-4ddf-9b33-9b0609081da0	Abbott Portfolio Analytics	abbott-portfolio-analytics	medium	Abbott's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	8798fdcf-0411-45cb-83dd-b4912e133354	2025-08-11 13:08:07.879318	0	0	0.00	\N	\N
d82ba28e-c5e9-4d4b-a868-9eb70cff8b13	Bristol Myers Portfolio Analytics	bristol-myers-portfolio-analytics	medium	Bristol Myers's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	47c2009b-81d2-458f-96b0-1a68aee370d6	2025-08-11 13:08:07.879929	0	0	0.00	\N	\N
f7c6524a-7892-49c7-8f3d-c6bf1f610d4f	Eli Lilly Portfolio Analytics	eli-lilly-portfolio-analytics	medium	Eli Lilly's investment team analyzes portfolio performance and risk metrics to optimize asset allocation strategies.	f	t	e1b879e5-e95b-41ee-b22a-a2ea91897277	2025-08-11 13:08:07.880514	0	0	0.00	\N	\N
c9150191-e1c2-428e-bfa7-10cd2aca4516	Goldman Sachs Risk Management System	goldman-sachs-risk-management-system	hard	Goldman Sachs's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.881142	0	0	0.00	\N	\N
21e44923-0f54-4782-937c-59917622752b	Morgan Stanley Risk Management System	morgan-stanley-risk-management-system	hard	Morgan Stanley's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.881675	0	0	0.00	\N	\N
f2ba73f5-2964-42ae-8705-2ea1aff2001b	Credit Suisse Risk Management System	credit-suisse-risk-management-system	hard	Credit Suisse's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.882172	0	0	0.00	\N	\N
614e2862-eac5-4ea0-96e3-f45f2c44d46e	UBS Risk Management System	ubs-risk-management-system	hard	UBS's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.882641	0	0	0.00	\N	\N
48c005a7-d8da-47b7-b266-8abc09bc7ba9	Deutsche Bank Risk Management System	deutsche-bank-risk-management-system	hard	Deutsche Bank's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.883443	0	0	0.00	\N	\N
35bf652f-f0c5-42b7-a1df-6c7936a7bf19	Barclays Risk Management System	barclays-risk-management-system	hard	Barclays's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.88398	0	0	0.00	\N	\N
e3b08029-a646-4855-97f3-ad8fc7a1b300	HSBC Risk Management System	hsbc-risk-management-system	hard	HSBC's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.884613	0	0	0.00	\N	\N
d52d9256-38a1-4a27-9d80-9cc3edb2e076	BNP Paribas Risk Management System	bnp-paribas-risk-management-system	hard	BNP Paribas's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.885656	0	0	0.00	\N	\N
a62c7efb-6dc5-40ae-8261-6cecdd2a482e	Societe Generale Risk Management System	societe-generale-risk-management-system	hard	Societe Generale's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.886657	0	0	0.00	\N	\N
1f1ecb19-0ca8-4831-918f-0385bda358bf	ING Group Risk Management System	ing-group-risk-management-system	hard	ING Group's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.887474	0	0	0.00	\N	\N
7f371be0-5b37-466d-b402-e780bc583742	Santander Risk Management System	santander-risk-management-system	hard	Santander's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.888333	0	0	0.00	\N	\N
ad4cdc97-b86c-47f2-aaf5-4bb66bab4b20	BBVA Risk Management System	bbva-risk-management-system	hard	BBVA's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.889039	0	0	0.00	\N	\N
8594b69b-4955-4f85-8f3b-cca9da9db830	UniCredit Risk Management System	unicredit-risk-management-system	hard	UniCredit's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.889826	0	0	0.00	\N	\N
130642c4-caa3-4fb9-8069-7b29b664b0b6	Intesa Sanpaolo Risk Management System	intesa-sanpaolo-risk-management-system	hard	Intesa Sanpaolo's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.890628	0	0	0.00	\N	\N
0ce766e5-5495-4f79-b455-308809cb24d1	Nordea Risk Management System	nordea-risk-management-system	hard	Nordea's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.891465	0	0	0.00	\N	\N
d8bd7205-68e2-4567-8db7-bb1613df16f8	SEB Risk Management System	seb-risk-management-system	hard	SEB's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.892396	0	0	0.00	\N	\N
076d0462-bff2-4b4b-a1fb-5f4fec111ea7	Danske Bank Risk Management System	danske-bank-risk-management-system	hard	Danske Bank's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.893023	0	0	0.00	\N	\N
d277eb23-10d8-4551-b82b-87b041897e84	ABN AMRO Risk Management System	abn-amro-risk-management-system	hard	ABN AMRO's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.893568	0	0	0.00	\N	\N
c5750767-06f3-4cb6-96b1-03de6ddfd977	KBC Group Risk Management System	kbc-group-risk-management-system	hard	KBC Group's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.894179	0	0	0.00	\N	\N
f6a716db-62ef-40fd-87c5-0bb2cceef980	Commerzbank Risk Management System	commerzbank-risk-management-system	hard	Commerzbank's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.894738	0	0	0.00	\N	\N
8fcc44d6-3682-414a-91d2-55c184b6b3d8	Rabobank Risk Management System	rabobank-risk-management-system	hard	Rabobank's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.89549	0	0	0.00	\N	\N
ee934c23-de4c-457a-afbe-e6bf602bbf78	Standard Chartered Risk Management System	standard-chartered-risk-management-system	hard	Standard Chartered's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.896208	0	0	0.00	\N	\N
c9929cd5-9715-469c-9293-b56b9a0e6b55	Lloyds Banking Risk Management System	lloyds-banking-risk-management-system	hard	Lloyds Banking's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.896816	0	0	0.00	\N	\N
62a8c083-7287-4f29-b924-057d61ea4397	Royal Bank Scotland Risk Management System	royal-bank-scotland-risk-management-system	hard	Royal Bank Scotland's quantitative risk team develops sophisticated models to measure and manage portfolio risk across multiple asset classes and market conditions.	f	t	c7e4c5a1-5b75-4117-a113-118749434557	2025-08-11 13:08:07.897403	0	0	0.00	\N	\N
\.


--
-- Name: categories categories_name_key; Type: CONSTRAINT; Schema: public; Owner: ss
--

ALTER TABLE ONLY public.categories
    ADD CONSTRAINT categories_name_key UNIQUE (name);


--
-- Name: categories categories_pkey; Type: CONSTRAINT; Schema: public; Owner: ss
--

ALTER TABLE ONLY public.categories
    ADD CONSTRAINT categories_pkey PRIMARY KEY (id);


--
-- Name: categories categories_slug_key; Type: CONSTRAINT; Schema: public; Owner: ss
--

ALTER TABLE ONLY public.categories
    ADD CONSTRAINT categories_slug_key UNIQUE (slug);


--
-- Name: problem_schemas problem_schemas_pkey; Type: CONSTRAINT; Schema: public; Owner: ss
--

ALTER TABLE ONLY public.problem_schemas
    ADD CONSTRAINT problem_schemas_pkey PRIMARY KEY (id);


--
-- Name: problems problems_pkey; Type: CONSTRAINT; Schema: public; Owner: ss
--

ALTER TABLE ONLY public.problems
    ADD CONSTRAINT problems_pkey PRIMARY KEY (id);


--
-- Name: problems problems_slug_key; Type: CONSTRAINT; Schema: public; Owner: ss
--

ALTER TABLE ONLY public.problems
    ADD CONSTRAINT problems_slug_key UNIQUE (slug);


--
-- Name: problem_schemas problem_schemas_problem_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ss
--

ALTER TABLE ONLY public.problem_schemas
    ADD CONSTRAINT problem_schemas_problem_id_fkey FOREIGN KEY (problem_id) REFERENCES public.problems(id) ON DELETE CASCADE;


--
-- Name: problems problems_category_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ss
--

ALTER TABLE ONLY public.problems
    ADD CONSTRAINT problems_category_id_fkey FOREIGN KEY (category_id) REFERENCES public.categories(id);


--
-- PostgreSQL database dump complete
--

